{"repository": {"owner": {"login": "keras-team"}, "name": "keras", "forkCount": 18728, "stargazerCount": 50841, "createdAt": "2015-03-28T00:35:42Z", "updatedAt": "2021-03-11T04:31:37Z", "repositoryTopics": {"edges": [{"node": {"topic": {"name": "deep-learning"}}}, {"node": {"topic": {"name": "tensorflow"}}}, {"node": {"topic": {"name": "neural-networks"}}}, {"node": {"topic": {"name": "machine-learning"}}}, {"node": {"topic": {"name": "data-science"}}}, {"node": {"topic": {"name": "python"}}}]}, "languages": {"edges": [{"node": {"name": "Starlark"}}, {"node": {"name": "Python"}}, {"node": {"name": "Shell"}}]}, "primaryLanguage": {"name": "Python"}}, "id": "MDU6SXNzdWUyMTI2MjQ2NzQ=", "number": 5644, "author": {"login": "mjdietzx"}, "title": "BatchNormalization bug when used in generator of a GAN", "body": "Easy to reproduce this bug when training GANs but probably occurs in other use cases as well. When `BatchNormalization` is used in the `generator` of a GAN, `combined.train_on_batch` fails. It's really weird but for some reason `combined.train_on_batch` does not calculate the loss correctly when batch norm is used in the generator. I tested the loss by hand doing something like:\r\n\r\n```python\r\nloss = combined.train_on_batch(x, y)\r\ncombined_pred = combined.predict(x)\r\nloss_check = K.eval(custom_loss(y, combined_pred))\r\n>>> loss != loss_check\r\n```\r\n\r\nYou can reproduce this in `wGAN` branch here: https://github.com/wayaai/GAN-Sandbox and uncomment the BatchNorm layer in the generator.", "bodyHTML": "<p>Easy to reproduce this bug when training GANs but probably occurs in other use cases as well. When <code>BatchNormalization</code> is used in the <code>generator</code> of a GAN, <code>combined.train_on_batch</code> fails. It's really weird but for some reason <code>combined.train_on_batch</code> does not calculate the loss correctly when batch norm is used in the generator. I tested the loss by hand doing something like:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-s1\">loss</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">combined</span>.<span class=\"pl-en\">train_on_batch</span>(<span class=\"pl-s1\">x</span>, <span class=\"pl-s1\">y</span>)\n<span class=\"pl-s1\">combined_pred</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">combined</span>.<span class=\"pl-en\">predict</span>(<span class=\"pl-s1\">x</span>)\n<span class=\"pl-s1\">loss_check</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">K</span>.<span class=\"pl-en\">eval</span>(<span class=\"pl-en\">custom_loss</span>(<span class=\"pl-s1\">y</span>, <span class=\"pl-s1\">combined_pred</span>))\n<span class=\"pl-c1\">&gt;&gt;</span><span class=\"pl-c1\">&gt;</span> <span class=\"pl-s1\">loss</span> <span class=\"pl-c1\">!=</span> <span class=\"pl-s1\">loss_check</span></pre></div>\n<p>You can reproduce this in <code>wGAN</code> branch here: <a href=\"https://github.com/wayaai/GAN-Sandbox\">https://github.com/wayaai/GAN-Sandbox</a> and uncomment the BatchNorm layer in the generator.</p>", "bodyText": "Easy to reproduce this bug when training GANs but probably occurs in other use cases as well. When BatchNormalization is used in the generator of a GAN, combined.train_on_batch fails. It's really weird but for some reason combined.train_on_batch does not calculate the loss correctly when batch norm is used in the generator. I tested the loss by hand doing something like:\nloss = combined.train_on_batch(x, y)\ncombined_pred = combined.predict(x)\nloss_check = K.eval(custom_loss(y, combined_pred))\n>>> loss != loss_check\nYou can reproduce this in wGAN branch here: https://github.com/wayaai/GAN-Sandbox and uncomment the BatchNorm layer in the generator.", "assignees": {"edges": []}, "createdAt": "2017-03-08T03:02:57Z", "closed": true, "closedAt": "2017-07-12T01:25:03Z", "lastEditedAt": null, "publishedAt": "2017-03-08T03:02:57Z", "comments": {"totalCount": 9, "edges": [{"node": {"id": "MDEyOklzc3VlQ29tbWVudDI4NTAxMDcwNw==", "author": {"login": "unrealwill"}, "body": "Hello,\r\n\r\nProbably a duplicate of : \r\nhttps://github.com/fchollet/keras/issues/5576", "bodyText": "Hello,\nProbably a duplicate of :\n#5576", "bodyHTML": "<p>Hello,</p>\n<p>Probably a duplicate of :<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"211363115\" data-permission-text=\"Title is private\" data-url=\"https://github.com/keras-team/keras/issues/5576\" data-hovercard-type=\"issue\" data-hovercard-url=\"/keras-team/keras/issues/5576/hovercard\" href=\"https://github.com/keras-team/keras/issues/5576\">#5576</a></p>", "createdAt": "2017-03-08T10:55:51Z", "publishedAt": "2017-03-08T10:55:51Z", "lastEditedAt": null, "updatedAt": "2017-03-08T10:55:51Z", "reactions": {"edges": []}, "bodyParsed": [{"number": 3902, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "Hello,", "sentSegmented": ["hello"], "sentSegmentedWithoutStops": ["hello"], "sentSegmentedWithoutStopsStemmed": ["hello"]}, {"number": 3903, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "Probably a duplicate of :", "sentSegmented": ["probably", "a", "duplicate", "of"], "sentSegmentedWithoutStops": ["probably", "duplicate"], "sentSegmentedWithoutStopsStemmed": ["probabl", "duplic"]}, {"number": 3904, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "#5576", "sentSegmented": ["5576"], "sentSegmentedWithoutStops": ["5576"], "sentSegmentedWithoutStopsStemmed": ["5576"]}]}}, {"node": {"id": "MDEyOklzc3VlQ29tbWVudDI4NTA5MjI1NQ==", "author": {"login": "mjdietzx"}, "body": "I don't think these issues are duplicated b/c this issue is when BN is used in generator and it seems to mess up the computation of the loss. The issue you referenced is when BN is used in discriminator and causes an expection to be raised b/c of problem in the computation graph. \r\n\r\nthis problem is really hard to discover and may be causing big problems behind the scenes without people realizing. I'm not sure if `predict_on_batch` and `train_on_batch` use different batch normalization statistics which could explain a small difference but this difference in loss seems to really blow up as training progresses which makes me think there is a big problem somewhere.\r\n\r\nI added test cases that reproduce this and show exactly what is going on:\r\nhttps://github.com/fchollet/keras/pull/5647", "bodyText": "I don't think these issues are duplicated b/c this issue is when BN is used in generator and it seems to mess up the computation of the loss. The issue you referenced is when BN is used in discriminator and causes an expection to be raised b/c of problem in the computation graph.\nthis problem is really hard to discover and may be causing big problems behind the scenes without people realizing. I'm not sure if predict_on_batch and train_on_batch use different batch normalization statistics which could explain a small difference but this difference in loss seems to really blow up as training progresses which makes me think there is a big problem somewhere.\nI added test cases that reproduce this and show exactly what is going on:\n#5647", "bodyHTML": "<p>I don't think these issues are duplicated b/c this issue is when BN is used in generator and it seems to mess up the computation of the loss. The issue you referenced is when BN is used in discriminator and causes an expection to be raised b/c of problem in the computation graph.</p>\n<p>this problem is really hard to discover and may be causing big problems behind the scenes without people realizing. I'm not sure if <code>predict_on_batch</code> and <code>train_on_batch</code> use different batch normalization statistics which could explain a small difference but this difference in loss seems to really blow up as training progresses which makes me think there is a big problem somewhere.</p>\n<p>I added test cases that reproduce this and show exactly what is going on:<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"212649851\" data-permission-text=\"Title is private\" data-url=\"https://github.com/keras-team/keras/issues/5647\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/keras-team/keras/pull/5647/hovercard\" href=\"https://github.com/keras-team/keras/pull/5647\">#5647</a></p>", "createdAt": "2017-03-08T16:34:10Z", "publishedAt": "2017-03-08T16:34:10Z", "lastEditedAt": null, "updatedAt": "2017-03-08T16:34:10Z", "reactions": {"edges": []}, "bodyParsed": [{"number": 3905, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "I don't think these issues are duplicated b/c this issue is when BN is used in generator and it seems to mess up the computation of the loss.", "sentSegmented": ["i", "do", "n't", "think", "these", "issues", "are", "duplicated", "b/c", "this", "issue", "is", "when", "bn", "is", "used", "in", "generator", "and", "it", "seems", "to", "mess", "up", "the", "computation", "of", "the", "loss"], "sentSegmentedWithoutStops": ["n't", "think", "issues", "duplicated", "b/c", "issue", "bn", "used", "generator", "seems", "mess", "computation", "loss"], "sentSegmentedWithoutStopsStemmed": ["n't", "think", "issu", "duplic", "b/c", "issu", "bn", "use", "gener", "seem", "mess", "comput", "loss"]}, {"number": 3906, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "The issue you referenced is when BN is used in discriminator and causes an expection to be raised b/c of problem in the computation graph.", "sentSegmented": ["the", "issue", "you", "referenced", "is", "when", "bn", "is", "used", "in", "discriminator", "and", "causes", "an", "expection", "to", "be", "raised", "b/c", "of", "problem", "in", "the", "computation", "graph"], "sentSegmentedWithoutStops": ["issue", "referenced", "bn", "used", "discriminator", "causes", "expection", "raised", "b/c", "problem", "computation", "graph"], "sentSegmentedWithoutStopsStemmed": ["issu", "referenc", "bn", "use", "discrimin", "caus", "expect", "rais", "b/c", "problem", "comput", "graph"]}, {"number": 3907, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "this problem is really hard to discover and may be causing big problems behind the scenes without people realizing.", "sentSegmented": ["this", "problem", "is", "really", "hard", "to", "discover", "and", "may", "be", "causing", "big", "problems", "behind", "the", "scenes", "without", "people", "realizing"], "sentSegmentedWithoutStops": ["problem", "really", "hard", "discover", "may", "causing", "big", "problems", "behind", "scenes", "without", "people", "realizing"], "sentSegmentedWithoutStopsStemmed": ["problem", "realli", "hard", "discov", "may", "caus", "big", "problem", "behind", "scene", "without", "peopl", "realiz"]}, {"number": 3908, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "I'm not sure if predict_on_batch and train_on_batch use different batch normalization statistics which could explain a small difference but this difference in loss seems to really blow up as training progresses which makes me think there is a big problem somewhere.", "sentSegmented": ["i", "'m", "not", "sure", "if", "predict_on_batch", "and", "train_on_batch", "use", "different", "batch", "normalization", "statistics", "which", "could", "explain", "a", "small", "difference", "but", "this", "difference", "in", "loss", "seems", "to", "really", "blow", "up", "as", "training", "progresses", "which", "makes", "me", "think", "there", "is", "a", "big", "problem", "somewhere"], "sentSegmentedWithoutStops": ["'m", "sure", "predict_on_batch", "train_on_batch", "use", "different", "batch", "normalization", "statistics", "could", "explain", "small", "difference", "difference", "loss", "seems", "really", "blow", "training", "progresses", "makes", "think", "big", "problem", "somewhere"], "sentSegmentedWithoutStopsStemmed": ["'m", "sure", "predict_on_batch", "train_on_batch", "use", "differ", "batch", "normal", "statist", "could", "explain", "small", "differ", "differ", "loss", "seem", "realli", "blow", "train", "progress", "make", "think", "big", "problem", "somewher"]}, {"number": 3909, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "I added test cases that reproduce this and show exactly what is going on:", "sentSegmented": ["i", "added", "test", "cases", "that", "reproduce", "this", "and", "show", "exactly", "what", "is", "going", "on"], "sentSegmentedWithoutStops": ["added", "test", "cases", "reproduce", "show", "exactly", "going"], "sentSegmentedWithoutStopsStemmed": ["ad", "test", "case", "reproduc", "show", "exactli", "go"]}, {"number": 3910, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "#5647", "sentSegmented": ["5647"], "sentSegmentedWithoutStops": ["5647"], "sentSegmentedWithoutStopsStemmed": ["5647"]}]}}, {"node": {"id": "MDEyOklzc3VlQ29tbWVudDI4NTEzOTkxOQ==", "author": {"login": "unrealwill"}, "body": "I'm not sure your test shows something.\r\nModels can have some regularization terms which will be added to the loss of the model, but obviously not added when you compute a custom_loss on the output of a model.\r\n\r\nAlso when you use predict the \"learningPhase\" variable is not the same as when in training, so the behaviour is expected to be different.\r\n\r\nhttps://github.com/fchollet/keras/blob/master/keras/engine/training.py#L1265\r\n\r\nKeras is currently in a PR freeze. \r\nMaybe some correction for Batch Normalization have already been made into the keras2 branch. \r\n", "bodyText": "I'm not sure your test shows something.\nModels can have some regularization terms which will be added to the loss of the model, but obviously not added when you compute a custom_loss on the output of a model.\nAlso when you use predict the \"learningPhase\" variable is not the same as when in training, so the behaviour is expected to be different.\nhttps://github.com/fchollet/keras/blob/master/keras/engine/training.py#L1265\nKeras is currently in a PR freeze.\nMaybe some correction for Batch Normalization have already been made into the keras2 branch.", "bodyHTML": "<p>I'm not sure your test shows something.<br>\nModels can have some regularization terms which will be added to the loss of the model, but obviously not added when you compute a custom_loss on the output of a model.</p>\n<p>Also when you use predict the \"learningPhase\" variable is not the same as when in training, so the behaviour is expected to be different.</p>\n<p><a href=\"https://github.com/fchollet/keras/blob/master/keras/engine/training.py#L1265\">https://github.com/fchollet/keras/blob/master/keras/engine/training.py#L1265</a></p>\n<p>Keras is currently in a PR freeze.<br>\nMaybe some correction for Batch Normalization have already been made into the keras2 branch.</p>", "createdAt": "2017-03-08T19:19:00Z", "publishedAt": "2017-03-08T19:19:00Z", "lastEditedAt": null, "updatedAt": "2017-03-08T19:19:00Z", "reactions": {"edges": []}, "bodyParsed": [{"number": 3911, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "I'm not sure your test shows something.", "sentSegmented": ["i", "'m", "not", "sure", "your", "test", "shows", "something"], "sentSegmentedWithoutStops": ["'m", "sure", "test", "shows", "something"], "sentSegmentedWithoutStopsStemmed": ["'m", "sure", "test", "show", "someth"]}, {"number": 3912, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "Models can have some regularization terms which will be added to the loss of the model, but obviously not added when you compute a custom_loss on the output of a model.", "sentSegmented": ["models", "can", "have", "some", "regularization", "terms", "which", "will", "be", "added", "to", "the", "loss", "of", "the", "model", "but", "obviously", "not", "added", "when", "you", "compute", "a", "custom_loss", "on", "the", "output", "of", "a", "model"], "sentSegmentedWithoutStops": ["models", "regularization", "terms", "added", "loss", "model", "obviously", "added", "compute", "custom_loss", "output", "model"], "sentSegmentedWithoutStopsStemmed": ["model", "regular", "term", "ad", "loss", "model", "obvious", "ad", "comput", "custom_loss", "output", "model"]}, {"number": 3913, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "Also when you use predict the \"learningPhase\" variable is not the same as when in training, so the behaviour is expected to be different.", "sentSegmented": ["also", "when", "you", "use", "predict", "the", "learningphase", "variable", "is", "not", "the", "same", "as", "when", "in", "training", "so", "the", "behaviour", "is", "expected", "to", "be", "different"], "sentSegmentedWithoutStops": ["also", "use", "predict", "learningphase", "variable", "training", "behaviour", "expected", "different"], "sentSegmentedWithoutStopsStemmed": ["also", "use", "predict", "learningphas", "variabl", "train", "behaviour", "expect", "differ"]}, {"number": 3914, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "https://github.com/fchollet/keras/blob/master/keras/engine/training.py#L1265", "sentSegmented": ["https", "//github.com/fchollet/keras/blob/master/keras/engine/training.py", "l1265"], "sentSegmentedWithoutStops": ["https", "//github.com/fchollet/keras/blob/master/keras/engine/training.py", "l1265"], "sentSegmentedWithoutStopsStemmed": ["http", "//github.com/fchollet/keras/blob/master/keras/engine/training.pi", "l1265"]}, {"number": 3915, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "Keras is currently in a PR freeze.", "sentSegmented": ["keras", "is", "currently", "in", "a", "pr", "freeze"], "sentSegmentedWithoutStops": ["keras", "currently", "pr", "freeze"], "sentSegmentedWithoutStopsStemmed": ["kera", "current", "pr", "freez"]}, {"number": 3916, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "Maybe some correction for Batch Normalization have already been made into the keras2 branch.", "sentSegmented": ["maybe", "some", "correction", "for", "batch", "normalization", "have", "already", "been", "made", "into", "the", "keras2", "branch"], "sentSegmentedWithoutStops": ["maybe", "correction", "batch", "normalization", "already", "made", "keras2", "branch"], "sentSegmentedWithoutStopsStemmed": ["mayb", "correct", "batch", "normal", "alreadi", "made", "keras2", "branch"]}]}}, {"node": {"id": "MDEyOklzc3VlQ29tbWVudDI4NTE0MjI1OA==", "author": {"login": "mjdietzx"}, "body": "I use keras-2 and see this problem in both versions. directed PR at keras-2.\r\n\r\nI'm not using any regularization (in my code where I see error) or in the tests so that shouldn't be a prob. I quickly looked through the code and it doesn't look like `learningPhase` should change the behavior of the simple models i included in tests (just a dense layer, batch norm, a dense layer and sigmoid activation). maybe something is going on that im missing?\r\n\r\nim hoping this test highlights the error im seeing. in my case i have a custom loss and more complicated models and the loss during training is very obviously wrong while if i manually compute the loss as done in tests it is correct (and as training progresses this difference increases a lot)", "bodyText": "I use keras-2 and see this problem in both versions. directed PR at keras-2.\nI'm not using any regularization (in my code where I see error) or in the tests so that shouldn't be a prob. I quickly looked through the code and it doesn't look like learningPhase should change the behavior of the simple models i included in tests (just a dense layer, batch norm, a dense layer and sigmoid activation). maybe something is going on that im missing?\nim hoping this test highlights the error im seeing. in my case i have a custom loss and more complicated models and the loss during training is very obviously wrong while if i manually compute the loss as done in tests it is correct (and as training progresses this difference increases a lot)", "bodyHTML": "<p>I use keras-2 and see this problem in both versions. directed PR at keras-2.</p>\n<p>I'm not using any regularization (in my code where I see error) or in the tests so that shouldn't be a prob. I quickly looked through the code and it doesn't look like <code>learningPhase</code> should change the behavior of the simple models i included in tests (just a dense layer, batch norm, a dense layer and sigmoid activation). maybe something is going on that im missing?</p>\n<p>im hoping this test highlights the error im seeing. in my case i have a custom loss and more complicated models and the loss during training is very obviously wrong while if i manually compute the loss as done in tests it is correct (and as training progresses this difference increases a lot)</p>", "createdAt": "2017-03-08T19:27:25Z", "publishedAt": "2017-03-08T19:27:25Z", "lastEditedAt": null, "updatedAt": "2017-03-08T19:27:25Z", "reactions": {"edges": []}, "bodyParsed": [{"number": 3917, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "I use keras-2 and see this problem in both versions.", "sentSegmented": ["i", "use", "keras-2", "and", "see", "this", "problem", "in", "both", "versions"], "sentSegmentedWithoutStops": ["use", "keras-2", "see", "problem", "versions"], "sentSegmentedWithoutStopsStemmed": ["use", "keras-2", "see", "problem", "version"]}, {"number": 3918, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "directed PR at keras-2.", "sentSegmented": ["directed", "pr", "at", "keras-2"], "sentSegmentedWithoutStops": ["directed", "pr", "keras-2"], "sentSegmentedWithoutStopsStemmed": ["direct", "pr", "keras-2"]}, {"number": 3919, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "I'm not using any regularization (in my code where I see error) or in the tests so that shouldn't be a prob.", "sentSegmented": ["i", "'m", "not", "using", "any", "regularization", "in", "my", "code", "where", "i", "see", "error", "or", "in", "the", "tests", "so", "that", "should", "n't", "be", "a", "prob"], "sentSegmentedWithoutStops": ["'m", "using", "regularization", "code", "see", "error", "tests", "n't", "prob"], "sentSegmentedWithoutStopsStemmed": ["'m", "use", "regular", "code", "see", "error", "test", "n't", "prob"]}, {"number": 3920, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "I quickly looked through the code and it doesn't look like learningPhase should change the behavior of the simple models i included in tests (just a dense layer, batch norm, a dense layer and sigmoid activation).", "sentSegmented": ["i", "quickly", "looked", "through", "the", "code", "and", "it", "does", "n't", "look", "like", "learningphase", "should", "change", "the", "behavior", "of", "the", "simple", "models", "i", "included", "in", "tests", "just", "a", "dense", "layer", "batch", "norm", "a", "dense", "layer", "and", "sigmoid", "activation"], "sentSegmentedWithoutStops": ["quickly", "looked", "code", "n't", "look", "like", "learningphase", "change", "behavior", "simple", "models", "included", "tests", "dense", "layer", "batch", "norm", "dense", "layer", "sigmoid", "activation"], "sentSegmentedWithoutStopsStemmed": ["quickli", "look", "code", "n't", "look", "like", "learningphas", "chang", "behavior", "simpl", "model", "includ", "test", "dens", "layer", "batch", "norm", "dens", "layer", "sigmoid", "activ"]}, {"number": 3921, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "maybe something is going on that im missing?", "sentSegmented": ["maybe", "something", "is", "going", "on", "that", "im", "missing"], "sentSegmentedWithoutStops": ["maybe", "something", "going", "im", "missing"], "sentSegmentedWithoutStopsStemmed": ["mayb", "someth", "go", "im", "miss"]}, {"number": 3922, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "im hoping this test highlights the error im seeing.", "sentSegmented": ["im", "hoping", "this", "test", "highlights", "the", "error", "im", "seeing"], "sentSegmentedWithoutStops": ["im", "hoping", "test", "highlights", "error", "im", "seeing"], "sentSegmentedWithoutStopsStemmed": ["im", "hope", "test", "highlight", "error", "im", "see"]}, {"number": 3923, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "in my case i have a custom loss and more complicated models and the loss during training is very obviously wrong while if i manually compute the loss as done in tests it is correct (and as training progresses this difference increases a lot)", "sentSegmented": ["in", "my", "case", "i", "have", "a", "custom", "loss", "and", "more", "complicated", "models", "and", "the", "loss", "during", "training", "is", "very", "obviously", "wrong", "while", "if", "i", "manually", "compute", "the", "loss", "as", "done", "in", "tests", "it", "is", "correct", "and", "as", "training", "progresses", "this", "difference", "increases", "a", "lot"], "sentSegmentedWithoutStops": ["case", "custom", "loss", "complicated", "models", "loss", "training", "obviously", "wrong", "manually", "compute", "loss", "done", "tests", "correct", "training", "progresses", "difference", "increases", "lot"], "sentSegmentedWithoutStopsStemmed": ["case", "custom", "loss", "complic", "model", "loss", "train", "obvious", "wrong", "manual", "comput", "loss", "done", "test", "correct", "train", "progress", "differ", "increas", "lot"]}]}}, {"node": {"id": "MDEyOklzc3VlQ29tbWVudDI4NTE0NjQwMA==", "author": {"login": "unrealwill"}, "body": "I don't know which mode you use for batchNormalization but in mode 0 (default mode)\r\nlearningPhase has an impact : \r\nhttps://github.com/fchollet/keras/blob/master/keras/layers/normalization.py#L143\r\n", "bodyText": "I don't know which mode you use for batchNormalization but in mode 0 (default mode)\nlearningPhase has an impact :\nhttps://github.com/fchollet/keras/blob/master/keras/layers/normalization.py#L143", "bodyHTML": "<p>I don't know which mode you use for batchNormalization but in mode 0 (default mode)<br>\nlearningPhase has an impact :<br>\n<a href=\"https://github.com/fchollet/keras/blob/master/keras/layers/normalization.py#L143\">https://github.com/fchollet/keras/blob/master/keras/layers/normalization.py#L143</a></p>", "createdAt": "2017-03-08T19:41:33Z", "publishedAt": "2017-03-08T19:41:33Z", "lastEditedAt": null, "updatedAt": "2017-03-08T19:41:33Z", "reactions": {"edges": []}, "bodyParsed": [{"number": 3924, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "I don't know which mode you use for batchNormalization but in mode 0 (default mode)", "sentSegmented": ["i", "do", "n't", "know", "which", "mode", "you", "use", "for", "batchnormalization", "but", "in", "mode", "0", "default", "mode"], "sentSegmentedWithoutStops": ["n't", "know", "mode", "use", "batchnormalization", "mode", "0", "default", "mode"], "sentSegmentedWithoutStopsStemmed": ["n't", "know", "mode", "use", "batchnorm", "mode", "0", "default", "mode"]}, {"number": 3925, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "learningPhase has an impact :", "sentSegmented": ["learningphase", "has", "an", "impact"], "sentSegmentedWithoutStops": ["learningphase", "impact"], "sentSegmentedWithoutStopsStemmed": ["learningphas", "impact"]}, {"number": 3926, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "https://github.com/fchollet/keras/blob/master/keras/layers/normalization.py#L143", "sentSegmented": ["https", "//github.com/fchollet/keras/blob/master/keras/layers/normalization.py", "l143"], "sentSegmentedWithoutStops": ["https", "//github.com/fchollet/keras/blob/master/keras/layers/normalization.py", "l143"], "sentSegmentedWithoutStopsStemmed": ["http", "//github.com/fchollet/keras/blob/master/keras/layers/normalization.pi", "l143"]}]}}, {"node": {"id": "MDEyOklzc3VlQ29tbWVudDI4NjAzMTQwMA==", "author": {"login": "hadarpo"}, "body": "I think that this problem does not depend on the specific model and is not due to regularization. Perhaps the reason is that batch normalization needs an epoch context in order to calculate statistics for normalization and hence does not behaves properly with train_on_batch (rather than fit or fit_generator)", "bodyText": "I think that this problem does not depend on the specific model and is not due to regularization. Perhaps the reason is that batch normalization needs an epoch context in order to calculate statistics for normalization and hence does not behaves properly with train_on_batch (rather than fit or fit_generator)", "bodyHTML": "<p>I think that this problem does not depend on the specific model and is not due to regularization. Perhaps the reason is that batch normalization needs an epoch context in order to calculate statistics for normalization and hence does not behaves properly with train_on_batch (rather than fit or fit_generator)</p>", "createdAt": "2017-03-13T07:17:00Z", "publishedAt": "2017-03-13T07:17:00Z", "lastEditedAt": null, "updatedAt": "2017-03-13T07:17:00Z", "reactions": {"edges": []}, "bodyParsed": [{"number": 3927, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "I think that this problem does not depend on the specific model and is not due to regularization.", "sentSegmented": ["i", "think", "that", "this", "problem", "does", "not", "depend", "on", "the", "specific", "model", "and", "is", "not", "due", "to", "regularization"], "sentSegmentedWithoutStops": ["think", "problem", "depend", "specific", "model", "due", "regularization"], "sentSegmentedWithoutStopsStemmed": ["think", "problem", "depend", "specif", "model", "due", "regular"]}, {"number": 3928, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "Perhaps the reason is that batch normalization needs an epoch context in order to calculate statistics for normalization and hence does not behaves properly with train_on_batch (rather than fit or fit_generator)", "sentSegmented": ["perhaps", "the", "reason", "is", "that", "batch", "normalization", "needs", "an", "epoch", "context", "in", "order", "to", "calculate", "statistics", "for", "normalization", "and", "hence", "does", "not", "behaves", "properly", "with", "train_on_batch", "rather", "than", "fit", "or", "fit_generator"], "sentSegmentedWithoutStops": ["perhaps", "reason", "batch", "normalization", "needs", "epoch", "context", "order", "calculate", "statistics", "normalization", "hence", "behaves", "properly", "train_on_batch", "rather", "fit", "fit_generator"], "sentSegmentedWithoutStopsStemmed": ["perhap", "reason", "batch", "normal", "need", "epoch", "context", "order", "calcul", "statist", "normal", "henc", "behav", "properli", "train_on_batch", "rather", "fit", "fit_gener"]}]}}, {"node": {"id": "MDEyOklzc3VlQ29tbWVudDMwNzY2OTQ1Mg==", "author": {"login": "stale"}, "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n", "bodyText": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.", "bodyHTML": "<p>This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.</p>", "createdAt": "2017-06-12T01:06:32Z", "publishedAt": "2017-06-12T01:06:32Z", "lastEditedAt": null, "updatedAt": "2017-06-12T01:06:32Z", "reactions": {"edges": []}, "bodyParsed": [{"number": 3929, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "This issue has been automatically marked as stale because it has not had recent activity.", "sentSegmented": ["this", "issue", "has", "been", "automatically", "marked", "as", "stale", "because", "it", "has", "not", "had", "recent", "activity"], "sentSegmentedWithoutStops": ["issue", "automatically", "marked", "stale", "recent", "activity"], "sentSegmentedWithoutStopsStemmed": ["issu", "automat", "mark", "stale", "recent", "activ"]}, {"number": 3930, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.", "sentSegmented": ["it", "will", "be", "closed", "after", "30", "days", "if", "no", "further", "activity", "occurs", "but", "feel", "free", "to", "re-open", "a", "closed", "issue", "if", "needed"], "sentSegmentedWithoutStops": ["closed", "30", "days", "activity", "occurs", "feel", "free", "re-open", "closed", "issue", "needed"], "sentSegmentedWithoutStopsStemmed": ["close", "30", "day", "activ", "occur", "feel", "free", "re-open", "close", "issu", "need"]}]}}, {"node": {"id": "MDEyOklzc3VlQ29tbWVudDMyMjQ1NTA1Mg==", "author": {"login": "oliblum90"}, "body": "I have the same problem. Is there a solution yet?", "bodyText": "I have the same problem. Is there a solution yet?", "bodyHTML": "<p>I have the same problem. Is there a solution yet?</p>", "createdAt": "2017-08-15T12:38:11Z", "publishedAt": "2017-08-15T12:38:11Z", "lastEditedAt": null, "updatedAt": "2017-08-15T12:38:11Z", "reactions": {"edges": []}, "bodyParsed": [{"number": 3931, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "I have the same problem.", "sentSegmented": ["i", "have", "the", "same", "problem"], "sentSegmentedWithoutStops": ["problem"], "sentSegmentedWithoutStopsStemmed": ["problem"]}, {"number": 3932, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "Is there a solution yet?", "sentSegmented": ["is", "there", "a", "solution", "yet"], "sentSegmentedWithoutStops": ["solution", "yet"], "sentSegmentedWithoutStopsStemmed": ["solut", "yet"]}]}}, {"node": {"id": "MDEyOklzc3VlQ29tbWVudDQwNTIzMDMzOA==", "author": {"login": "HitLuca"}, "body": "Can confirm the issue, in order to fix this problem you can clone the BatchNormalizationClass and in the call() method change \r\n`return K.in_train_phase(normed_training, normalize_inference, training=training)`\r\ninto\r\n`return K.in_train_phase(normed_training, normalize_inference, training=True)`", "bodyText": "Can confirm the issue, in order to fix this problem you can clone the BatchNormalizationClass and in the call() method change\nreturn K.in_train_phase(normed_training, normalize_inference, training=training)\ninto\nreturn K.in_train_phase(normed_training, normalize_inference, training=True)", "bodyHTML": "<p>Can confirm the issue, in order to fix this problem you can clone the BatchNormalizationClass and in the call() method change<br>\n<code>return K.in_train_phase(normed_training, normalize_inference, training=training)</code><br>\ninto<br>\n<code>return K.in_train_phase(normed_training, normalize_inference, training=True)</code></p>", "createdAt": "2018-07-16T12:24:13Z", "publishedAt": "2018-07-16T12:24:13Z", "lastEditedAt": null, "updatedAt": "2018-07-16T12:24:13Z", "reactions": {"edges": []}, "bodyParsed": [{"number": 3933, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "Can confirm the issue, in order to fix this problem you can clone the BatchNormalizationClass and in the call() method change", "sentSegmented": ["can", "confirm", "the", "issue", "in", "order", "to", "fix", "this", "problem", "you", "can", "clone", "the", "batchnormalizationclass", "and", "in", "the", "call", "method", "change"], "sentSegmentedWithoutStops": ["confirm", "issue", "order", "fix", "problem", "clone", "batchnormalizationclass", "call", "method", "change"], "sentSegmentedWithoutStopsStemmed": ["confirm", "issu", "order", "fix", "problem", "clone", "batchnormalizationclass", "call", "method", "chang"]}, {"number": 3934, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "return K.in_train_phase(normed_training, normalize_inference, training=training)", "sentSegmented": ["return", "k.in_train_phase", "normed_training", "normalize_inference", "training=training"], "sentSegmentedWithoutStops": ["return", "k.in_train_phase", "normed_training", "normalize_inference", "training=training"], "sentSegmentedWithoutStopsStemmed": ["return", "k.in_train_phas", "normed_train", "normalize_infer", "training=train"]}, {"number": 3935, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "into", "sentSegmented": ["into"], "sentSegmentedWithoutStops": [], "sentSegmentedWithoutStopsStemmed": []}, {"number": 3936, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "return K.in_train_phase(normed_training, normalize_inference, training=True)", "sentSegmented": ["return", "k.in_train_phase", "normed_training", "normalize_inference", "training=true"], "sentSegmentedWithoutStops": ["return", "k.in_train_phase", "normed_training", "normalize_inference", "training=true"], "sentSegmentedWithoutStopsStemmed": ["return", "k.in_train_phas", "normed_train", "normalize_infer", "training=tru"]}]}}], "pageInfo": {"endCursor": "Y3Vyc29yOnYyOpHOGCdTAg==", "hasNextPage": false}}, "labels": {"edges": [{"node": {"createdAt": "2017-05-23T18:28:56Z", "name": "stale"}}]}, "milestone": null, "reactions": {"edges": [{"node": {"content": "THUMBS_UP", "createdAt": "2017-05-05T04:45:35Z"}}]}, "state": "CLOSED", "titleSegmented": ["batchnormalization", "bug", "when", "used", "in", "generator", "of", "a", "gan"], "titleSegmentedWithoutStops": ["batchnormalization", "bug", "used", "generator", "gan"], "titleSegmentedWithoutStopsStemmed": ["batchnorm", "bug", "use", "gener", "gan"], "bodyParsed": [{"number": 3893, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "Easy to reproduce this bug when training GANs but probably occurs in other use cases as well.", "sentSegmented": ["easy", "to", "reproduce", "this", "bug", "when", "training", "gans", "but", "probably", "occurs", "in", "other", "use", "cases", "as", "well"], "sentSegmentedWithoutStops": ["easy", "reproduce", "bug", "training", "gans", "probably", "occurs", "use", "cases", "well"], "sentSegmentedWithoutStopsStemmed": ["easi", "reproduc", "bug", "train", "gan", "probabl", "occur", "use", "case", "well"]}, {"number": 3894, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "When BatchNormalization is used in the generator of a GAN, combined.train_on_batch fails.", "sentSegmented": ["when", "batchnormalization", "is", "used", "in", "the", "generator", "of", "a", "gan", "combined.train_on_batch", "fails"], "sentSegmentedWithoutStops": ["batchnormalization", "used", "generator", "gan", "combined.train_on_batch", "fails"], "sentSegmentedWithoutStopsStemmed": ["batchnorm", "use", "gener", "gan", "combined.train_on_batch", "fail"]}, {"number": 3895, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "It's really weird but for some reason combined.train_on_batch does not calculate the loss correctly when batch norm is used in the generator.", "sentSegmented": ["it", "'s", "really", "weird", "but", "for", "some", "reason", "combined.train_on_batch", "does", "not", "calculate", "the", "loss", "correctly", "when", "batch", "norm", "is", "used", "in", "the", "generator"], "sentSegmentedWithoutStops": ["'s", "really", "weird", "reason", "combined.train_on_batch", "calculate", "loss", "correctly", "batch", "norm", "used", "generator"], "sentSegmentedWithoutStopsStemmed": ["'s", "realli", "weird", "reason", "combined.train_on_batch", "calcul", "loss", "correctli", "batch", "norm", "use", "gener"]}, {"number": 3896, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "I tested the loss by hand doing something like:", "sentSegmented": ["i", "tested", "the", "loss", "by", "hand", "doing", "something", "like"], "sentSegmentedWithoutStops": ["tested", "loss", "hand", "something", "like"], "sentSegmentedWithoutStopsStemmed": ["test", "loss", "hand", "someth", "like"]}, {"number": 3897, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "loss = combined.train_on_batch(x, y)", "sentSegmented": ["loss", "combined.train_on_batch", "x", "y"], "sentSegmentedWithoutStops": ["loss", "combined.train_on_batch", "x"], "sentSegmentedWithoutStopsStemmed": ["loss", "combined.train_on_batch", "x"]}, {"number": 3898, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "combined_pred = combined.predict(x)", "sentSegmented": ["combined_pred", "combined.predict", "x"], "sentSegmentedWithoutStops": ["combined_pred", "combined.predict", "x"], "sentSegmentedWithoutStopsStemmed": ["combined_pr", "combined.predict", "x"]}, {"number": 3899, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "loss_check = K.eval(custom_loss(y, combined_pred))", "sentSegmented": ["loss_check", "k.eval", "custom_loss", "y", "combined_pred"], "sentSegmentedWithoutStops": ["loss_check", "k.eval", "custom_loss", "combined_pred"], "sentSegmentedWithoutStopsStemmed": ["loss_check", "k.eval", "custom_loss", "combined_pr"]}, {"number": 3900, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": ">>> loss != loss_check", "sentSegmented": ["loss", "loss_check"], "sentSegmentedWithoutStops": ["loss", "loss_check"], "sentSegmentedWithoutStopsStemmed": ["loss", "loss_check"]}, {"number": 3901, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "You can reproduce this in wGAN branch here: https://github.com/wayaai/GAN-Sandbox and uncomment the BatchNorm layer in the generator.", "sentSegmented": ["you", "can", "reproduce", "this", "in", "wgan", "branch", "here", "https", "//github.com/wayaai/gan-sandbox", "and", "uncomment", "the", "batchnorm", "layer", "in", "the", "generator"], "sentSegmentedWithoutStops": ["reproduce", "wgan", "branch", "https", "//github.com/wayaai/gan-sandbox", "uncomment", "batchnorm", "layer", "generator"], "sentSegmentedWithoutStopsStemmed": ["reproduc", "wgan", "branch", "http", "//github.com/wayaai/gan-sandbox", "uncom", "batchnorm", "layer", "gener"]}]}