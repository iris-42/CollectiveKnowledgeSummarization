{"repository": {"owner": {"login": "keras-team"}, "name": "keras", "forkCount": 18725, "stargazerCount": 50877, "createdAt": "2015-03-28T00:35:42Z", "updatedAt": "2021-03-17T09:46:01Z", "repositoryTopics": {"edges": [{"node": {"topic": {"name": "deep-learning"}}}, {"node": {"topic": {"name": "tensorflow"}}}, {"node": {"topic": {"name": "neural-networks"}}}, {"node": {"topic": {"name": "machine-learning"}}}, {"node": {"topic": {"name": "data-science"}}}, {"node": {"topic": {"name": "python"}}}]}, "languages": {"edges": [{"node": {"name": "Starlark"}}, {"node": {"name": "Python"}}, {"node": {"name": "Shell"}}]}, "primaryLanguage": {"name": "Python"}}, "id": "MDU6SXNzdWUxODUwNDEzOTE=", "number": 4178, "author": {"login": "sjebbara"}, "title": "Problem with TimeDistributed() and Learning Phase", "body": "(**EDIT:** The following issue is only a minimal example of how to produce the error. My actual goal is to use a more complicated model instead of `Dropout()` here.)\n\nWhen executing the following script a `MissingInputError` occurs:\n\n``` python\nfrom keras.models import Model\nfrom keras.layers import Input, TimeDistributed, Dropout\n\nin1 = Input(batch_shape=(10, 8, 6), name=\"in1\")\nout1 = TimeDistributed(Dropout(0.5))(in1)\n\nmodel = Model(input=in1, output=out1)\nmodel.compile(\"adam\", \"mse\")\nmodel._make_predict_function()\n```\n\nThis is the simplest model that produces the error (In my original architecture, I tried to distribute a more complex model). The same issue occurs when replacing the `Dropout()` layer with e.g. `GaussianNoise()`, `GRU(dropout_W=0.5)`, but not for e.g. `Dense()`. I think the error boils down to the combination of `TimeDistributed()` and any layer (or model) that uses the learning phase.\n\nMaybe there is a conceptual problem with `TimeDistributed()` and the learning phase input?\n\nThese issues seem to be somewhat related: #3834, #2609, #3686, #2391\n\nThe full stack trace is this:\n\n```\n... \n  File \"/homes/sjebbara/git/keras-original/keras/engine/training.py\", line 752, in _make_predict_function\n    **kwargs)\n  File \"/homes/sjebbara/git/keras-original/keras/backend/theano_backend.py\", line 787, in function\n    return Function(inputs, outputs, updates=updates, **kwargs)\n  File \"/homes/sjebbara/git/keras-original/keras/backend/theano_backend.py\", line 773, in __init__\n    **kwargs)\n  File \"/homes/sjebbara/.local/lib/python2.7/site-packages/Theano-0.9.0.dev3-py2.7.egg/theano/compile/function.py\", line 326, in function\n    output_keys=output_keys)\n  File \"/homes/sjebbara/.local/lib/python2.7/site-packages/Theano-0.9.0.dev3-py2.7.egg/theano/compile/pfunc.py\", line 486, in pfunc\n    output_keys=output_keys)\n  File \"/homes/sjebbara/.local/lib/python2.7/site-packages/Theano-0.9.0.dev3-py2.7.egg/theano/compile/function_module.py\", line 1776, in orig_function\n    output_keys=output_keys).create(\n  File \"/homes/sjebbara/.local/lib/python2.7/site-packages/Theano-0.9.0.dev3-py2.7.egg/theano/compile/function_module.py\", line 1430, in __init__\n    accept_inplace)\n  File \"/homes/sjebbara/.local/lib/python2.7/site-packages/Theano-0.9.0.dev3-py2.7.egg/theano/compile/function_module.py\", line 176, in std_fgraph\n    update_mapping=update_mapping)\n  File \"/homes/sjebbara/.local/lib/python2.7/site-packages/Theano-0.9.0.dev3-py2.7.egg/theano/gof/fg.py\", line 180, in __init__\n    self.__import_r__(output, reason=\"init\")\n  File \"/homes/sjebbara/.local/lib/python2.7/site-packages/Theano-0.9.0.dev3-py2.7.egg/theano/gof/fg.py\", line 351, in __import_r__\n    self.__import__(variable.owner, reason=reason)\n  File \"/homes/sjebbara/.local/lib/python2.7/site-packages/Theano-0.9.0.dev3-py2.7.egg/theano/gof/fg.py\", line 396, in __import__\n    variable=r)\ntheano.gof.fg.MissingInputError: An input of the graph, used to compute Shape(<TensorType(float32, matrix)>), was not provided and not given a value.Use the Theano flag exception_verbosity='high',for more information on this error.\n\nBacktrace when the variable is created:\n  File \"/homes/sjebbara/PyCharmProjects/NeuralSentiment/src/Test2.py\", line 5, in <module>\n    out1 = TimeDistributed(Dropout(0.5))(in1)\n  File \"/homes/sjebbara/git/keras-original/keras/engine/topology.py\", line 514, in __call__\n    self.add_inbound_node(inbound_layers, node_indices, tensor_indices)\n  File \"/homes/sjebbara/git/keras-original/keras/engine/topology.py\", line 572, in add_inbound_node\n    Node.create_node(self, inbound_layers, node_indices, tensor_indices)\n  File \"/homes/sjebbara/git/keras-original/keras/engine/topology.py\", line 149, in create_node\n    output_tensors = to_list(outbound_layer.call(input_tensors[0], mask=input_masks[0]))\n  File \"/homes/sjebbara/git/keras-original/keras/layers/wrappers.py\", line 131, in call\n    initial_states=[], input_length=input_length, unroll=unroll)\n  File \"/homes/sjebbara/git/keras-original/keras/backend/theano_backend.py\", line 947, in rnn\n    go_backwards=go_backwards)\n```\n\n---\n\nPlease make sure that the boxes below are checked before you submit your issue. Thank you!\n- [ x] Check that you are up-to-date with the master branch of Keras. You can update with:\n  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps\n- [x ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:\n  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps\n- [x ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).\n", "bodyHTML": "<p>(<strong>EDIT:</strong> The following issue is only a minimal example of how to produce the error. My actual goal is to use a more complicated model instead of <code>Dropout()</code> here.)</p>\n<p>When executing the following script a <code>MissingInputError</code> occurs:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> <span class=\"pl-s1\">keras</span>.<span class=\"pl-s1\">models</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">Model</span>\n<span class=\"pl-k\">from</span> <span class=\"pl-s1\">keras</span>.<span class=\"pl-s1\">layers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">Input</span>, <span class=\"pl-v\">TimeDistributed</span>, <span class=\"pl-v\">Dropout</span>\n\n<span class=\"pl-s1\">in1</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Input</span>(<span class=\"pl-s1\">batch_shape</span><span class=\"pl-c1\">=</span>(<span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">8</span>, <span class=\"pl-c1\">6</span>), <span class=\"pl-s1\">name</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">\"in1\"</span>)\n<span class=\"pl-s1\">out1</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TimeDistributed</span>(<span class=\"pl-v\">Dropout</span>(<span class=\"pl-c1\">0.5</span>))(<span class=\"pl-s1\">in1</span>)\n\n<span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Model</span>(<span class=\"pl-s1\">input</span><span class=\"pl-c1\">=</span><span class=\"pl-s1\">in1</span>, <span class=\"pl-s1\">output</span><span class=\"pl-c1\">=</span><span class=\"pl-s1\">out1</span>)\n<span class=\"pl-s1\">model</span>.<span class=\"pl-en\">compile</span>(<span class=\"pl-s\">\"adam\"</span>, <span class=\"pl-s\">\"mse\"</span>)\n<span class=\"pl-s1\">model</span>.<span class=\"pl-en\">_make_predict_function</span>()</pre></div>\n<p>This is the simplest model that produces the error (In my original architecture, I tried to distribute a more complex model). The same issue occurs when replacing the <code>Dropout()</code> layer with e.g. <code>GaussianNoise()</code>, <code>GRU(dropout_W=0.5)</code>, but not for e.g. <code>Dense()</code>. I think the error boils down to the combination of <code>TimeDistributed()</code> and any layer (or model) that uses the learning phase.</p>\n<p>Maybe there is a conceptual problem with <code>TimeDistributed()</code> and the learning phase input?</p>\n<p>These issues seem to be somewhat related: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"178342859\" data-permission-text=\"Title is private\" data-url=\"https://github.com/keras-team/keras/issues/3834\" data-hovercard-type=\"issue\" data-hovercard-url=\"/keras-team/keras/issues/3834/hovercard\" href=\"https://github.com/keras-team/keras/issues/3834\">#3834</a>, <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"153019075\" data-permission-text=\"Title is private\" data-url=\"https://github.com/keras-team/keras/issues/2609\" data-hovercard-type=\"issue\" data-hovercard-url=\"/keras-team/keras/issues/2609/hovercard\" href=\"https://github.com/keras-team/keras/issues/2609\">#2609</a>, <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"174958454\" data-permission-text=\"Title is private\" data-url=\"https://github.com/keras-team/keras/issues/3686\" data-hovercard-type=\"issue\" data-hovercard-url=\"/keras-team/keras/issues/3686/hovercard\" href=\"https://github.com/keras-team/keras/issues/3686\">#3686</a>, <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"149333634\" data-permission-text=\"Title is private\" data-url=\"https://github.com/keras-team/keras/issues/2391\" data-hovercard-type=\"issue\" data-hovercard-url=\"/keras-team/keras/issues/2391/hovercard\" href=\"https://github.com/keras-team/keras/issues/2391\">#2391</a></p>\n<p>The full stack trace is this:</p>\n<pre><code>... \n  File \"/homes/sjebbara/git/keras-original/keras/engine/training.py\", line 752, in _make_predict_function\n    **kwargs)\n  File \"/homes/sjebbara/git/keras-original/keras/backend/theano_backend.py\", line 787, in function\n    return Function(inputs, outputs, updates=updates, **kwargs)\n  File \"/homes/sjebbara/git/keras-original/keras/backend/theano_backend.py\", line 773, in __init__\n    **kwargs)\n  File \"/homes/sjebbara/.local/lib/python2.7/site-packages/Theano-0.9.0.dev3-py2.7.egg/theano/compile/function.py\", line 326, in function\n    output_keys=output_keys)\n  File \"/homes/sjebbara/.local/lib/python2.7/site-packages/Theano-0.9.0.dev3-py2.7.egg/theano/compile/pfunc.py\", line 486, in pfunc\n    output_keys=output_keys)\n  File \"/homes/sjebbara/.local/lib/python2.7/site-packages/Theano-0.9.0.dev3-py2.7.egg/theano/compile/function_module.py\", line 1776, in orig_function\n    output_keys=output_keys).create(\n  File \"/homes/sjebbara/.local/lib/python2.7/site-packages/Theano-0.9.0.dev3-py2.7.egg/theano/compile/function_module.py\", line 1430, in __init__\n    accept_inplace)\n  File \"/homes/sjebbara/.local/lib/python2.7/site-packages/Theano-0.9.0.dev3-py2.7.egg/theano/compile/function_module.py\", line 176, in std_fgraph\n    update_mapping=update_mapping)\n  File \"/homes/sjebbara/.local/lib/python2.7/site-packages/Theano-0.9.0.dev3-py2.7.egg/theano/gof/fg.py\", line 180, in __init__\n    self.__import_r__(output, reason=\"init\")\n  File \"/homes/sjebbara/.local/lib/python2.7/site-packages/Theano-0.9.0.dev3-py2.7.egg/theano/gof/fg.py\", line 351, in __import_r__\n    self.__import__(variable.owner, reason=reason)\n  File \"/homes/sjebbara/.local/lib/python2.7/site-packages/Theano-0.9.0.dev3-py2.7.egg/theano/gof/fg.py\", line 396, in __import__\n    variable=r)\ntheano.gof.fg.MissingInputError: An input of the graph, used to compute Shape(&lt;TensorType(float32, matrix)&gt;), was not provided and not given a value.Use the Theano flag exception_verbosity='high',for more information on this error.\n\nBacktrace when the variable is created:\n  File \"/homes/sjebbara/PyCharmProjects/NeuralSentiment/src/Test2.py\", line 5, in &lt;module&gt;\n    out1 = TimeDistributed(Dropout(0.5))(in1)\n  File \"/homes/sjebbara/git/keras-original/keras/engine/topology.py\", line 514, in __call__\n    self.add_inbound_node(inbound_layers, node_indices, tensor_indices)\n  File \"/homes/sjebbara/git/keras-original/keras/engine/topology.py\", line 572, in add_inbound_node\n    Node.create_node(self, inbound_layers, node_indices, tensor_indices)\n  File \"/homes/sjebbara/git/keras-original/keras/engine/topology.py\", line 149, in create_node\n    output_tensors = to_list(outbound_layer.call(input_tensors[0], mask=input_masks[0]))\n  File \"/homes/sjebbara/git/keras-original/keras/layers/wrappers.py\", line 131, in call\n    initial_states=[], input_length=input_length, unroll=unroll)\n  File \"/homes/sjebbara/git/keras-original/keras/backend/theano_backend.py\", line 947, in rnn\n    go_backwards=go_backwards)\n</code></pre>\n<hr>\n<p>Please make sure that the boxes below are checked before you submit your issue. Thank you!</p>\n<ul>\n<li>[ x] Check that you are up-to-date with the master branch of Keras. You can update with:<br>\npip install git+git://github.com/fchollet/keras.git --upgrade --no-deps</li>\n<li>[x ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:<br>\npip install git+git://github.com/Theano/Theano.git --upgrade --no-deps</li>\n<li>[x ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).</li>\n</ul>", "bodyText": "(EDIT: The following issue is only a minimal example of how to produce the error. My actual goal is to use a more complicated model instead of Dropout() here.)\nWhen executing the following script a MissingInputError occurs:\nfrom keras.models import Model\nfrom keras.layers import Input, TimeDistributed, Dropout\n\nin1 = Input(batch_shape=(10, 8, 6), name=\"in1\")\nout1 = TimeDistributed(Dropout(0.5))(in1)\n\nmodel = Model(input=in1, output=out1)\nmodel.compile(\"adam\", \"mse\")\nmodel._make_predict_function()\nThis is the simplest model that produces the error (In my original architecture, I tried to distribute a more complex model). The same issue occurs when replacing the Dropout() layer with e.g. GaussianNoise(), GRU(dropout_W=0.5), but not for e.g. Dense(). I think the error boils down to the combination of TimeDistributed() and any layer (or model) that uses the learning phase.\nMaybe there is a conceptual problem with TimeDistributed() and the learning phase input?\nThese issues seem to be somewhat related: #3834, #2609, #3686, #2391\nThe full stack trace is this:\n... \n  File \"/homes/sjebbara/git/keras-original/keras/engine/training.py\", line 752, in _make_predict_function\n    **kwargs)\n  File \"/homes/sjebbara/git/keras-original/keras/backend/theano_backend.py\", line 787, in function\n    return Function(inputs, outputs, updates=updates, **kwargs)\n  File \"/homes/sjebbara/git/keras-original/keras/backend/theano_backend.py\", line 773, in __init__\n    **kwargs)\n  File \"/homes/sjebbara/.local/lib/python2.7/site-packages/Theano-0.9.0.dev3-py2.7.egg/theano/compile/function.py\", line 326, in function\n    output_keys=output_keys)\n  File \"/homes/sjebbara/.local/lib/python2.7/site-packages/Theano-0.9.0.dev3-py2.7.egg/theano/compile/pfunc.py\", line 486, in pfunc\n    output_keys=output_keys)\n  File \"/homes/sjebbara/.local/lib/python2.7/site-packages/Theano-0.9.0.dev3-py2.7.egg/theano/compile/function_module.py\", line 1776, in orig_function\n    output_keys=output_keys).create(\n  File \"/homes/sjebbara/.local/lib/python2.7/site-packages/Theano-0.9.0.dev3-py2.7.egg/theano/compile/function_module.py\", line 1430, in __init__\n    accept_inplace)\n  File \"/homes/sjebbara/.local/lib/python2.7/site-packages/Theano-0.9.0.dev3-py2.7.egg/theano/compile/function_module.py\", line 176, in std_fgraph\n    update_mapping=update_mapping)\n  File \"/homes/sjebbara/.local/lib/python2.7/site-packages/Theano-0.9.0.dev3-py2.7.egg/theano/gof/fg.py\", line 180, in __init__\n    self.__import_r__(output, reason=\"init\")\n  File \"/homes/sjebbara/.local/lib/python2.7/site-packages/Theano-0.9.0.dev3-py2.7.egg/theano/gof/fg.py\", line 351, in __import_r__\n    self.__import__(variable.owner, reason=reason)\n  File \"/homes/sjebbara/.local/lib/python2.7/site-packages/Theano-0.9.0.dev3-py2.7.egg/theano/gof/fg.py\", line 396, in __import__\n    variable=r)\ntheano.gof.fg.MissingInputError: An input of the graph, used to compute Shape(<TensorType(float32, matrix)>), was not provided and not given a value.Use the Theano flag exception_verbosity='high',for more information on this error.\n\nBacktrace when the variable is created:\n  File \"/homes/sjebbara/PyCharmProjects/NeuralSentiment/src/Test2.py\", line 5, in <module>\n    out1 = TimeDistributed(Dropout(0.5))(in1)\n  File \"/homes/sjebbara/git/keras-original/keras/engine/topology.py\", line 514, in __call__\n    self.add_inbound_node(inbound_layers, node_indices, tensor_indices)\n  File \"/homes/sjebbara/git/keras-original/keras/engine/topology.py\", line 572, in add_inbound_node\n    Node.create_node(self, inbound_layers, node_indices, tensor_indices)\n  File \"/homes/sjebbara/git/keras-original/keras/engine/topology.py\", line 149, in create_node\n    output_tensors = to_list(outbound_layer.call(input_tensors[0], mask=input_masks[0]))\n  File \"/homes/sjebbara/git/keras-original/keras/layers/wrappers.py\", line 131, in call\n    initial_states=[], input_length=input_length, unroll=unroll)\n  File \"/homes/sjebbara/git/keras-original/keras/backend/theano_backend.py\", line 947, in rnn\n    go_backwards=go_backwards)\n\n\nPlease make sure that the boxes below are checked before you submit your issue. Thank you!\n\n[ x] Check that you are up-to-date with the master branch of Keras. You can update with:\npip install git+git://github.com/fchollet/keras.git --upgrade --no-deps\n[x ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:\npip install git+git://github.com/Theano/Theano.git --upgrade --no-deps\n[x ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).", "assignees": {"edges": []}, "createdAt": "2016-10-25T07:42:08Z", "closed": true, "closedAt": "2017-08-18T15:43:39Z", "lastEditedAt": "2016-10-26T08:37:48Z", "publishedAt": "2016-10-25T07:42:08Z", "comments": {"edges": [{"node": {"id": "MDEyOklzc3VlQ29tbWVudDI1NTk5Mjk4MA==", "author": {"login": "kudkudak"}, "body": "I think you have incorrectly applied TimeDistributed to Dropout. \n\n`TimeDistributed(Dropout(0.5))(in1)` should be `TimeDistributed(Dropout(0.5)(in1))(in1)`\n", "bodyText": "I think you have incorrectly applied TimeDistributed to Dropout.\nTimeDistributed(Dropout(0.5))(in1) should be TimeDistributed(Dropout(0.5)(in1))(in1)", "bodyHTML": "<p>I think you have incorrectly applied TimeDistributed to Dropout.</p>\n<p><code>TimeDistributed(Dropout(0.5))(in1)</code> should be <code>TimeDistributed(Dropout(0.5)(in1))(in1)</code></p>", "createdAt": "2016-10-25T10:05:23Z", "publishedAt": "2016-10-25T10:05:23Z", "lastEditedAt": null, "updatedAt": "2016-10-25T10:05:23Z", "reactions": {"edges": []}, "bodyParsed": [{"number": 1216, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "I think you have incorrectly applied TimeDistributed to Dropout.", "sentSegmented": ["i", "think", "you", "have", "incorrectly", "applied", "timedistributed", "to", "dropout"], "sentSegmentedWithoutStops": ["think", "incorrectly", "applied", "timedistributed", "dropout"], "sentSegmentedWithoutStopsStemmed": ["think", "incorrectli", "appli", "timedistribut", "dropout"]}, {"number": 1217, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "TimeDistributed(Dropout(0.5))(in1) should be TimeDistributed(Dropout(0.5)(in1))(in1)", "sentSegmented": ["timedistributed", "dropout", "0.5", "in1", "should", "be", "timedistributed", "dropout", "0.5", "in1", "in1"], "sentSegmentedWithoutStops": ["timedistributed", "dropout", "0.5", "in1", "timedistributed", "dropout", "0.5", "in1", "in1"], "sentSegmentedWithoutStopsStemmed": ["timedistribut", "dropout", "0.5", "in1", "timedistribut", "dropout", "0.5", "in1", "in1"]}]}}, {"node": {"id": "MDEyOklzc3VlQ29tbWVudDI1NjAwOTE2Ng==", "author": {"login": "sjebbara"}, "body": "Thanks for the reply.\nI am quiet sure that the `TimeDistributed()` layer expects a `Layer` object and not a tensor (which `Dropout(0.5)(in1)` would return).\nAlso, when changing \n`out1 = TimeDistributed(Dropout(0.5))(in1)` to\n`out1 = TimeDistributed(Dense(10))(in1)`\neverything works fine. \n", "bodyText": "Thanks for the reply.\nI am quiet sure that the TimeDistributed() layer expects a Layer object and not a tensor (which Dropout(0.5)(in1) would return).\nAlso, when changing\nout1 = TimeDistributed(Dropout(0.5))(in1) to\nout1 = TimeDistributed(Dense(10))(in1)\neverything works fine.", "bodyHTML": "<p>Thanks for the reply.<br>\nI am quiet sure that the <code>TimeDistributed()</code> layer expects a <code>Layer</code> object and not a tensor (which <code>Dropout(0.5)(in1)</code> would return).<br>\nAlso, when changing<br>\n<code>out1 = TimeDistributed(Dropout(0.5))(in1)</code> to<br>\n<code>out1 = TimeDistributed(Dense(10))(in1)</code><br>\neverything works fine.</p>", "createdAt": "2016-10-25T11:27:35Z", "publishedAt": "2016-10-25T11:27:35Z", "lastEditedAt": "2016-10-26T18:26:24Z", "updatedAt": "2016-10-26T18:26:24Z", "reactions": {"edges": []}, "bodyParsed": [{"number": 1218, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "Thanks for the reply.", "sentSegmented": ["thanks", "for", "the", "reply"], "sentSegmentedWithoutStops": ["thanks", "reply"], "sentSegmentedWithoutStopsStemmed": ["thank", "repli"]}, {"number": 1219, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "I am quiet sure that the TimeDistributed() layer expects a Layer object and not a tensor (which Dropout(0.5)(in1) would return).", "sentSegmented": ["i", "am", "quiet", "sure", "that", "the", "timedistributed", "layer", "expects", "a", "layer", "object", "and", "not", "a", "tensor", "which", "dropout", "0.5", "in1", "would", "return"], "sentSegmentedWithoutStops": ["quiet", "sure", "timedistributed", "layer", "expects", "layer", "object", "tensor", "dropout", "0.5", "in1", "would", "return"], "sentSegmentedWithoutStopsStemmed": ["quiet", "sure", "timedistribut", "layer", "expect", "layer", "object", "tensor", "dropout", "0.5", "in1", "would", "return"]}, {"number": 1220, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "Also, when changing", "sentSegmented": ["also", "when", "changing"], "sentSegmentedWithoutStops": ["also", "changing"], "sentSegmentedWithoutStopsStemmed": ["also", "chang"]}, {"number": 1221, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "out1 = TimeDistributed(Dropout(0.5))(in1) to", "sentSegmented": ["out1", "timedistributed", "dropout", "0.5", "in1", "to"], "sentSegmentedWithoutStops": ["out1", "timedistributed", "dropout", "0.5", "in1"], "sentSegmentedWithoutStopsStemmed": ["out1", "timedistribut", "dropout", "0.5", "in1"]}, {"number": 1222, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "out1 = TimeDistributed(Dense(10))(in1)", "sentSegmented": ["out1", "timedistributed", "dense", "10", "in1"], "sentSegmentedWithoutStops": ["out1", "timedistributed", "dense", "10", "in1"], "sentSegmentedWithoutStopsStemmed": ["out1", "timedistribut", "dens", "10", "in1"]}, {"number": 1223, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "everything works fine.", "sentSegmented": ["everything", "works", "fine"], "sentSegmentedWithoutStops": ["everything", "works", "fine"], "sentSegmentedWithoutStopsStemmed": ["everyth", "work", "fine"]}]}}, {"node": {"id": "MDEyOklzc3VlQ29tbWVudDI1NjE2NTYxNg==", "author": {"login": "farizrahman4u"}, "body": "This is a bug in theano when `RandomStreams` are present inside a `scan` op. See: https://groups.google.com/forum/#!topic/theano-users/8diyZjq6ngc\n\nSolutions : \n- Don't provide `batch_size` or,\n- Don't use `TimeDistributed` over `Dropout`. `TimeDistributed(Dropout(0.5))(x)` and `Dropout(0.5)(x)` are equivalent.\n\nIf you are trying to drop the same set of nodes for all timesteps in a sequence, simply wrapping in `TimeDistributed` will not do the job. See my solution at #3995 \n", "bodyText": "This is a bug in theano when RandomStreams are present inside a scan op. See: https://groups.google.com/forum/#!topic/theano-users/8diyZjq6ngc\nSolutions :\n\nDon't provide batch_size or,\nDon't use TimeDistributed over Dropout. TimeDistributed(Dropout(0.5))(x) and Dropout(0.5)(x) are equivalent.\n\nIf you are trying to drop the same set of nodes for all timesteps in a sequence, simply wrapping in TimeDistributed will not do the job. See my solution at #3995", "bodyHTML": "<p>This is a bug in theano when <code>RandomStreams</code> are present inside a <code>scan</code> op. See: <a rel=\"nofollow\" href=\"https://groups.google.com/forum/#!topic/theano-users/8diyZjq6ngc\">https://groups.google.com/forum/#!topic/theano-users/8diyZjq6ngc</a></p>\n<p>Solutions :</p>\n<ul>\n<li>Don't provide <code>batch_size</code> or,</li>\n<li>Don't use <code>TimeDistributed</code> over <code>Dropout</code>. <code>TimeDistributed(Dropout(0.5))(x)</code> and <code>Dropout(0.5)(x)</code> are equivalent.</li>\n</ul>\n<p>If you are trying to drop the same set of nodes for all timesteps in a sequence, simply wrapping in <code>TimeDistributed</code> will not do the job. See my solution at <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"181792878\" data-permission-text=\"Title is private\" data-url=\"https://github.com/keras-team/keras/issues/3995\" data-hovercard-type=\"issue\" data-hovercard-url=\"/keras-team/keras/issues/3995/hovercard\" href=\"https://github.com/keras-team/keras/issues/3995\">#3995</a></p>", "createdAt": "2016-10-25T20:27:33Z", "publishedAt": "2016-10-25T20:27:33Z", "lastEditedAt": "2016-10-25T20:30:43Z", "updatedAt": "2016-10-25T20:30:43Z", "reactions": {"edges": []}, "bodyParsed": [{"number": 1224, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "This is a bug in theano when RandomStreams are present inside a scan op.", "sentSegmented": ["this", "is", "a", "bug", "in", "theano", "when", "randomstreams", "are", "present", "inside", "a", "scan", "op"], "sentSegmentedWithoutStops": ["bug", "theano", "randomstreams", "present", "inside", "scan", "op"], "sentSegmentedWithoutStopsStemmed": ["bug", "theano", "randomstream", "present", "insid", "scan", "op"]}, {"number": 1225, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "See: https://groups.google.com/forum/#!topic/theano-users/8diyZjq6ngc", "sentSegmented": ["see", "https", "//groups.google.com/forum/", "topic/theano-users/8diyzjq6ngc"], "sentSegmentedWithoutStops": ["see", "https", "//groups.google.com/forum/", "topic/theano-users/8diyzjq6ngc"], "sentSegmentedWithoutStopsStemmed": ["see", "http", "//groups.google.com/forum/", "topic/theano-users/8diyzjq6ngc"]}, {"number": 1226, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "Solutions :", "sentSegmented": ["solutions"], "sentSegmentedWithoutStops": ["solutions"], "sentSegmentedWithoutStopsStemmed": ["solut"]}, {"number": 1227, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "Don't provide batch_size or,", "sentSegmented": ["do", "n't", "provide", "batch_size", "or"], "sentSegmentedWithoutStops": ["n't", "provide", "batch_size"], "sentSegmentedWithoutStopsStemmed": ["n't", "provid", "batch_siz"]}, {"number": 1228, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "Don't use TimeDistributed over Dropout.", "sentSegmented": ["do", "n't", "use", "timedistributed", "over", "dropout"], "sentSegmentedWithoutStops": ["n't", "use", "timedistributed", "dropout"], "sentSegmentedWithoutStopsStemmed": ["n't", "use", "timedistribut", "dropout"]}, {"number": 1229, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "TimeDistributed(Dropout(0.5))(x) and Dropout(0.5)(x) are equivalent.", "sentSegmented": ["timedistributed", "dropout", "0.5", "x", "and", "dropout", "0.5", "x", "are", "equivalent"], "sentSegmentedWithoutStops": ["timedistributed", "dropout", "0.5", "x", "dropout", "0.5", "x", "equivalent"], "sentSegmentedWithoutStopsStemmed": ["timedistribut", "dropout", "0.5", "x", "dropout", "0.5", "x", "equival"]}, {"number": 1230, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "If you are trying to drop the same set of nodes for all timesteps in a sequence, simply wrapping in TimeDistributed will not do the job.", "sentSegmented": ["if", "you", "are", "trying", "to", "drop", "the", "same", "set", "of", "nodes", "for", "all", "timesteps", "in", "a", "sequence", "simply", "wrapping", "in", "timedistributed", "will", "not", "do", "the", "job"], "sentSegmentedWithoutStops": ["trying", "drop", "set", "nodes", "timesteps", "sequence", "simply", "wrapping", "timedistributed", "job"], "sentSegmentedWithoutStopsStemmed": ["tri", "drop", "set", "node", "timestep", "sequenc", "simpli", "wrap", "timedistribut", "job"]}, {"number": 1231, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "See my solution at #3995", "sentSegmented": ["see", "my", "solution", "at", "3995"], "sentSegmentedWithoutStops": ["see", "solution", "3995"], "sentSegmentedWithoutStopsStemmed": ["see", "solut", "3995"]}]}}, {"node": {"id": "MDEyOklzc3VlQ29tbWVudDI1NjI3MzQxNg==", "author": {"login": "sjebbara"}, "body": "Thanks for the pointer, @farizrahman4u .\n\nThe solutions that you suggested sadly do not apply to my real use case (which I simplified for this Issue). My actual goal is to have an inner model:\n\n``` python\ninner_in1 = Input(batch_shape=(batch_size, n_elements, element_size), name=\"inner_in1\")\ninner_output = GRU(2, dropout_U=0.5, dropout_W=0.5, return_sequences=False, name=\"gru\")(inner_in1)\ninner_model = Model(input=inner_in1, output=inner_output, name=\"inner_model\")\n```\n\nthat I use in a `TimeDistributed()` layer inside an outer model:\n\n``` python\nouter_in1 = Input(batch_shape=(batch_size, n_sequences, n_elements, element_size), name=\"outer_in1\")\nouter_output = TimeDistributedModel(inner_model, name=\"distr\")(outer_in1)\nouter_output = SomeOtherComputations()(outer_output)\nouter_model = Model(input=outer_in1, output=outer_output, name=\"outer_model\")\nouter_model.compile(\"adam\", \"mse\")\nouter_model._make_predict_function()\n```\n\nYou could see this as a sentence model (`inner_model`) that I apply to each sentence in a document (`outer_model`). In this setup, the error appears when using `dropout_W` or `dropout_U` in the GRU.\n\nNot specifying `batch_size` is not possible here, since [these lines in the TimeDistributed() layer](https://github.com/fchollet/keras/blob/master/keras/layers/wrappers.py#L133-L145) wouldn't make much sense with an RNN.\n", "bodyText": "Thanks for the pointer, @farizrahman4u .\nThe solutions that you suggested sadly do not apply to my real use case (which I simplified for this Issue). My actual goal is to have an inner model:\ninner_in1 = Input(batch_shape=(batch_size, n_elements, element_size), name=\"inner_in1\")\ninner_output = GRU(2, dropout_U=0.5, dropout_W=0.5, return_sequences=False, name=\"gru\")(inner_in1)\ninner_model = Model(input=inner_in1, output=inner_output, name=\"inner_model\")\nthat I use in a TimeDistributed() layer inside an outer model:\nouter_in1 = Input(batch_shape=(batch_size, n_sequences, n_elements, element_size), name=\"outer_in1\")\nouter_output = TimeDistributedModel(inner_model, name=\"distr\")(outer_in1)\nouter_output = SomeOtherComputations()(outer_output)\nouter_model = Model(input=outer_in1, output=outer_output, name=\"outer_model\")\nouter_model.compile(\"adam\", \"mse\")\nouter_model._make_predict_function()\nYou could see this as a sentence model (inner_model) that I apply to each sentence in a document (outer_model). In this setup, the error appears when using dropout_W or dropout_U in the GRU.\nNot specifying batch_size is not possible here, since these lines in the TimeDistributed() layer wouldn't make much sense with an RNN.", "bodyHTML": "<p>Thanks for the pointer, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/farizrahman4u/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/farizrahman4u\">@farizrahman4u</a> .</p>\n<p>The solutions that you suggested sadly do not apply to my real use case (which I simplified for this Issue). My actual goal is to have an inner model:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-s1\">inner_in1</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Input</span>(<span class=\"pl-s1\">batch_shape</span><span class=\"pl-c1\">=</span>(<span class=\"pl-s1\">batch_size</span>, <span class=\"pl-s1\">n_elements</span>, <span class=\"pl-s1\">element_size</span>), <span class=\"pl-s1\">name</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">\"inner_in1\"</span>)\n<span class=\"pl-s1\">inner_output</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">GRU</span>(<span class=\"pl-c1\">2</span>, <span class=\"pl-s1\">dropout_U</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">0.5</span>, <span class=\"pl-s1\">dropout_W</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">0.5</span>, <span class=\"pl-s1\">return_sequences</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-s1\">name</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">\"gru\"</span>)(<span class=\"pl-s1\">inner_in1</span>)\n<span class=\"pl-s1\">inner_model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Model</span>(<span class=\"pl-s1\">input</span><span class=\"pl-c1\">=</span><span class=\"pl-s1\">inner_in1</span>, <span class=\"pl-s1\">output</span><span class=\"pl-c1\">=</span><span class=\"pl-s1\">inner_output</span>, <span class=\"pl-s1\">name</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">\"inner_model\"</span>)</pre></div>\n<p>that I use in a <code>TimeDistributed()</code> layer inside an outer model:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-s1\">outer_in1</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Input</span>(<span class=\"pl-s1\">batch_shape</span><span class=\"pl-c1\">=</span>(<span class=\"pl-s1\">batch_size</span>, <span class=\"pl-s1\">n_sequences</span>, <span class=\"pl-s1\">n_elements</span>, <span class=\"pl-s1\">element_size</span>), <span class=\"pl-s1\">name</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">\"outer_in1\"</span>)\n<span class=\"pl-s1\">outer_output</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TimeDistributedModel</span>(<span class=\"pl-s1\">inner_model</span>, <span class=\"pl-s1\">name</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">\"distr\"</span>)(<span class=\"pl-s1\">outer_in1</span>)\n<span class=\"pl-s1\">outer_output</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">SomeOtherComputations</span>()(<span class=\"pl-s1\">outer_output</span>)\n<span class=\"pl-s1\">outer_model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Model</span>(<span class=\"pl-s1\">input</span><span class=\"pl-c1\">=</span><span class=\"pl-s1\">outer_in1</span>, <span class=\"pl-s1\">output</span><span class=\"pl-c1\">=</span><span class=\"pl-s1\">outer_output</span>, <span class=\"pl-s1\">name</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">\"outer_model\"</span>)\n<span class=\"pl-s1\">outer_model</span>.<span class=\"pl-en\">compile</span>(<span class=\"pl-s\">\"adam\"</span>, <span class=\"pl-s\">\"mse\"</span>)\n<span class=\"pl-s1\">outer_model</span>.<span class=\"pl-en\">_make_predict_function</span>()</pre></div>\n<p>You could see this as a sentence model (<code>inner_model</code>) that I apply to each sentence in a document (<code>outer_model</code>). In this setup, the error appears when using <code>dropout_W</code> or <code>dropout_U</code> in the GRU.</p>\n<p>Not specifying <code>batch_size</code> is not possible here, since <a href=\"https://github.com/fchollet/keras/blob/master/keras/layers/wrappers.py#L133-L145\">these lines in the TimeDistributed() layer</a> wouldn't make much sense with an RNN.</p>", "createdAt": "2016-10-26T07:44:50Z", "publishedAt": "2016-10-26T07:44:50Z", "lastEditedAt": null, "updatedAt": "2016-10-26T07:44:50Z", "reactions": {"edges": []}, "bodyParsed": [{"number": 1232, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "Thanks for the pointer, @farizrahman4u .", "sentSegmented": ["thanks", "for", "the", "pointer", "farizrahman4u"], "sentSegmentedWithoutStops": ["thanks", "pointer", "farizrahman4u"], "sentSegmentedWithoutStopsStemmed": ["thank", "pointer", "farizrahman4u"]}, {"number": 1233, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "The solutions that you suggested sadly do not apply to my real use case (which I simplified for this Issue).", "sentSegmented": ["the", "solutions", "that", "you", "suggested", "sadly", "do", "not", "apply", "to", "my", "real", "use", "case", "which", "i", "simplified", "for", "this", "issue"], "sentSegmentedWithoutStops": ["solutions", "suggested", "sadly", "apply", "real", "use", "case", "simplified", "issue"], "sentSegmentedWithoutStopsStemmed": ["solut", "suggest", "sadli", "appli", "real", "use", "case", "simplifi", "issu"]}, {"number": 1234, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "My actual goal is to have an inner model:", "sentSegmented": ["my", "actual", "goal", "is", "to", "have", "an", "inner", "model"], "sentSegmentedWithoutStops": ["actual", "goal", "inner", "model"], "sentSegmentedWithoutStopsStemmed": ["actual", "goal", "inner", "model"]}, {"number": 1235, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "inner_in1 = Input(batch_shape=(batch_size, n_elements, element_size), name=\"inner_in1\")", "sentSegmented": ["inner_in1", "input", "batch_shape=", "batch_size", "n_elements", "element_size", "name=", "inner_in1"], "sentSegmentedWithoutStops": ["inner_in1", "input", "batch_shape=", "batch_size", "n_elements", "element_size", "name=", "inner_in1"], "sentSegmentedWithoutStopsStemmed": ["inner_in1", "input", "batch_shape=", "batch_siz", "n_element", "element_s", "name=", "inner_in1"]}, {"number": 1236, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "inner_output = GRU(2, dropout_U=0.5, dropout_W=0.5, return_sequences=False, name=\"gru\")(inner_in1)", "sentSegmented": ["inner_output", "gru", "2", "dropout_u=0.5", "dropout_w=0.5", "return_sequences=false", "name=", "gru", "inner_in1"], "sentSegmentedWithoutStops": ["inner_output", "gru", "2", "dropout_u=0.5", "dropout_w=0.5", "return_sequences=false", "name=", "gru", "inner_in1"], "sentSegmentedWithoutStopsStemmed": ["inner_output", "gru", "2", "dropout_u=0.5", "dropout_w=0.5", "return_sequences=fals", "name=", "gru", "inner_in1"]}, {"number": 1237, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "inner_model = Model(input=inner_in1, output=inner_output, name=\"inner_model\")", "sentSegmented": ["inner_model", "model", "input=inner_in1", "output=inner_output", "name=", "inner_model"], "sentSegmentedWithoutStops": ["inner_model", "model", "input=inner_in1", "output=inner_output", "name=", "inner_model"], "sentSegmentedWithoutStopsStemmed": ["inner_model", "model", "input=inner_in1", "output=inner_output", "name=", "inner_model"]}, {"number": 1238, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "that I use in a TimeDistributed() layer inside an outer model:", "sentSegmented": ["that", "i", "use", "in", "a", "timedistributed", "layer", "inside", "an", "outer", "model"], "sentSegmentedWithoutStops": ["use", "timedistributed", "layer", "inside", "outer", "model"], "sentSegmentedWithoutStopsStemmed": ["use", "timedistribut", "layer", "insid", "outer", "model"]}, {"number": 1239, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "outer_in1 = Input(batch_shape=(batch_size, n_sequences, n_elements, element_size), name=\"outer_in1\")", "sentSegmented": ["outer_in1", "input", "batch_shape=", "batch_size", "n_sequences", "n_elements", "element_size", "name=", "outer_in1"], "sentSegmentedWithoutStops": ["outer_in1", "input", "batch_shape=", "batch_size", "n_sequences", "n_elements", "element_size", "name=", "outer_in1"], "sentSegmentedWithoutStopsStemmed": ["outer_in1", "input", "batch_shape=", "batch_siz", "n_sequenc", "n_element", "element_s", "name=", "outer_in1"]}, {"number": 1240, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "outer_output = TimeDistributedModel(inner_model, name=\"distr\")(outer_in1)", "sentSegmented": ["outer_output", "timedistributedmodel", "inner_model", "name=", "distr", "outer_in1"], "sentSegmentedWithoutStops": ["outer_output", "timedistributedmodel", "inner_model", "name=", "distr", "outer_in1"], "sentSegmentedWithoutStopsStemmed": ["outer_output", "timedistributedmodel", "inner_model", "name=", "distr", "outer_in1"]}, {"number": 1241, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "outer_output = SomeOtherComputations()(outer_output)", "sentSegmented": ["outer_output", "someothercomputations", "outer_output"], "sentSegmentedWithoutStops": ["outer_output", "someothercomputations", "outer_output"], "sentSegmentedWithoutStopsStemmed": ["outer_output", "someothercomput", "outer_output"]}, {"number": 1242, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "outer_model = Model(input=outer_in1, output=outer_output, name=\"outer_model\")", "sentSegmented": ["outer_model", "model", "input=outer_in1", "output=outer_output", "name=", "outer_model"], "sentSegmentedWithoutStops": ["outer_model", "model", "input=outer_in1", "output=outer_output", "name=", "outer_model"], "sentSegmentedWithoutStopsStemmed": ["outer_model", "model", "input=outer_in1", "output=outer_output", "name=", "outer_model"]}, {"number": 1243, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "outer_model.compile(\"adam\", \"mse\")", "sentSegmented": ["outer_model.compile", "adam", "mse"], "sentSegmentedWithoutStops": ["outer_model.compile", "adam", "mse"], "sentSegmentedWithoutStopsStemmed": ["outer_model.compil", "adam", "mse"]}, {"number": 1244, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "outer_model._make_predict_function()", "sentSegmented": ["outer_model._make_predict_function"], "sentSegmentedWithoutStops": ["outer_model._make_predict_function"], "sentSegmentedWithoutStopsStemmed": ["outer_model._make_predict_funct"]}, {"number": 1245, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "You could see this as a sentence model (inner_model) that I apply to each sentence in a document (outer_model).", "sentSegmented": ["you", "could", "see", "this", "as", "a", "sentence", "model", "inner_model", "that", "i", "apply", "to", "each", "sentence", "in", "a", "document", "outer_model"], "sentSegmentedWithoutStops": ["could", "see", "sentence", "model", "inner_model", "apply", "sentence", "document", "outer_model"], "sentSegmentedWithoutStopsStemmed": ["could", "see", "sentenc", "model", "inner_model", "appli", "sentenc", "document", "outer_model"]}, {"number": 1246, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "In this setup, the error appears when using dropout_W or dropout_U in the GRU.", "sentSegmented": ["in", "this", "setup", "the", "error", "appears", "when", "using", "dropout_w", "or", "dropout_u", "in", "the", "gru"], "sentSegmentedWithoutStops": ["setup", "error", "appears", "using", "dropout_w", "dropout_u", "gru"], "sentSegmentedWithoutStopsStemmed": ["setup", "error", "appear", "use", "dropout_w", "dropout_u", "gru"]}, {"number": 1247, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "Not specifying batch_size is not possible here, since these lines in the TimeDistributed() layer wouldn't make much sense with an RNN.", "sentSegmented": ["not", "specifying", "batch_size", "is", "not", "possible", "here", "since", "these", "lines", "in", "the", "timedistributed", "layer", "would", "n't", "make", "much", "sense", "with", "an", "rnn"], "sentSegmentedWithoutStops": ["specifying", "batch_size", "possible", "since", "lines", "timedistributed", "layer", "would", "n't", "make", "much", "sense", "rnn"], "sentSegmentedWithoutStopsStemmed": ["specifi", "batch_siz", "possibl", "sinc", "line", "timedistribut", "layer", "would", "n't", "make", "much", "sens", "rnn"]}]}}, {"node": {"id": "MDEyOklzc3VlQ29tbWVudDI1NjM1NjI5Mg==", "author": {"login": "eyaler"}, "body": "@farizrahman4u as i reported in https://github.com/fchollet/keras/issues/4182\n1. batch_size is required when using stateful rnn\n2. what i want to get in timedistributed(dropout) is _not_ the same dropout nodes for every timestep, but having every timestep drop exactly x% of nodes. without timedistributed you would get different fractions for different timesteps\n", "bodyText": "@farizrahman4u as i reported in #4182\n\nbatch_size is required when using stateful rnn\nwhat i want to get in timedistributed(dropout) is not the same dropout nodes for every timestep, but having every timestep drop exactly x% of nodes. without timedistributed you would get different fractions for different timesteps", "bodyHTML": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/farizrahman4u/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/farizrahman4u\">@farizrahman4u</a> as i reported in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"185164858\" data-permission-text=\"Title is private\" data-url=\"https://github.com/keras-team/keras/issues/4182\" data-hovercard-type=\"issue\" data-hovercard-url=\"/keras-team/keras/issues/4182/hovercard\" href=\"https://github.com/keras-team/keras/issues/4182\">#4182</a></p>\n<ol>\n<li>batch_size is required when using stateful rnn</li>\n<li>what i want to get in timedistributed(dropout) is <em>not</em> the same dropout nodes for every timestep, but having every timestep drop exactly x% of nodes. without timedistributed you would get different fractions for different timesteps</li>\n</ol>", "createdAt": "2016-10-26T14:00:38Z", "publishedAt": "2016-10-26T14:00:38Z", "lastEditedAt": null, "updatedAt": "2016-10-26T14:00:38Z", "reactions": {"edges": []}, "bodyParsed": [{"number": 1248, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "@farizrahman4u as i reported in #4182", "sentSegmented": ["farizrahman4u", "as", "i", "reported", "in", "4182"], "sentSegmentedWithoutStops": ["farizrahman4u", "reported", "4182"], "sentSegmentedWithoutStopsStemmed": ["farizrahman4u", "report", "4182"]}, {"number": 1249, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "batch_size is required when using stateful rnn", "sentSegmented": ["batch_size", "is", "required", "when", "using", "stateful", "rnn"], "sentSegmentedWithoutStops": ["batch_size", "required", "using", "stateful", "rnn"], "sentSegmentedWithoutStopsStemmed": ["batch_siz", "requir", "use", "state", "rnn"]}, {"number": 1250, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "what i want to get in timedistributed(dropout) is not the same dropout nodes for every timestep, but having every timestep drop exactly x% of nodes.", "sentSegmented": ["what", "i", "want", "to", "get", "in", "timedistributed", "dropout", "is", "not", "the", "same", "dropout", "nodes", "for", "every", "timestep", "but", "having", "every", "timestep", "drop", "exactly", "x", "of", "nodes"], "sentSegmentedWithoutStops": ["want", "get", "timedistributed", "dropout", "dropout", "nodes", "every", "timestep", "every", "timestep", "drop", "exactly", "x", "nodes"], "sentSegmentedWithoutStopsStemmed": ["want", "get", "timedistribut", "dropout", "dropout", "node", "everi", "timestep", "everi", "timestep", "drop", "exactli", "x", "node"]}, {"number": 1251, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "without timedistributed you would get different fractions for different timesteps", "sentSegmented": ["without", "timedistributed", "you", "would", "get", "different", "fractions", "for", "different", "timesteps"], "sentSegmentedWithoutStops": ["without", "timedistributed", "would", "get", "different", "fractions", "different", "timesteps"], "sentSegmentedWithoutStopsStemmed": ["without", "timedistribut", "would", "get", "differ", "fraction", "differ", "timestep"]}]}}, {"node": {"id": "MDEyOklzc3VlQ29tbWVudDI1NjM2NzcxNA==", "author": {"login": "farizrahman4u"}, "body": "@sjebbara There is no reason for you to provide the `batch_size` unless you are having a stateful RNN. Both rnn based and reshape based TimeDistributed implementations are strictly mathematically equivalent. (reshape based implementation being faster). If you still want to specify `batch_size`, here you go:\n\n``` python\nouter_in1 = Input(batch_shape=(batch_size, n_sequences, n_elements, element_size), name=\"outer_in1\")\nTimeDistributedModel = TimeDistributed(inner_model, name=\"distr\")\nTimeDistributedModel.build((None,) + outer_in1._keras_shape[1:])\nTimeDistributedModel.build = lambda *_: None\nouter_output = TimeDistributedModel(outer_in1)\nouter_output = SomeOtherComputations()(outer_output)\nouter_model = Model(input=outer_in1, output=outer_output, name=\"outer_model\")\nouter_model.compile(\"adam\", \"mse\")\nouter_model._make_predict_function()\n```\n", "bodyText": "@sjebbara There is no reason for you to provide the batch_size unless you are having a stateful RNN. Both rnn based and reshape based TimeDistributed implementations are strictly mathematically equivalent. (reshape based implementation being faster). If you still want to specify batch_size, here you go:\nouter_in1 = Input(batch_shape=(batch_size, n_sequences, n_elements, element_size), name=\"outer_in1\")\nTimeDistributedModel = TimeDistributed(inner_model, name=\"distr\")\nTimeDistributedModel.build((None,) + outer_in1._keras_shape[1:])\nTimeDistributedModel.build = lambda *_: None\nouter_output = TimeDistributedModel(outer_in1)\nouter_output = SomeOtherComputations()(outer_output)\nouter_model = Model(input=outer_in1, output=outer_output, name=\"outer_model\")\nouter_model.compile(\"adam\", \"mse\")\nouter_model._make_predict_function()", "bodyHTML": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/sjebbara/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/sjebbara\">@sjebbara</a> There is no reason for you to provide the <code>batch_size</code> unless you are having a stateful RNN. Both rnn based and reshape based TimeDistributed implementations are strictly mathematically equivalent. (reshape based implementation being faster). If you still want to specify <code>batch_size</code>, here you go:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-s1\">outer_in1</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Input</span>(<span class=\"pl-s1\">batch_shape</span><span class=\"pl-c1\">=</span>(<span class=\"pl-s1\">batch_size</span>, <span class=\"pl-s1\">n_sequences</span>, <span class=\"pl-s1\">n_elements</span>, <span class=\"pl-s1\">element_size</span>), <span class=\"pl-s1\">name</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">\"outer_in1\"</span>)\n<span class=\"pl-v\">TimeDistributedModel</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TimeDistributed</span>(<span class=\"pl-s1\">inner_model</span>, <span class=\"pl-s1\">name</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">\"distr\"</span>)\n<span class=\"pl-v\">TimeDistributedModel</span>.<span class=\"pl-en\">build</span>((<span class=\"pl-c1\">None</span>,) <span class=\"pl-c1\">+</span> <span class=\"pl-s1\">outer_in1</span>.<span class=\"pl-s1\">_keras_shape</span>[<span class=\"pl-c1\">1</span>:])\n<span class=\"pl-v\">TimeDistributedModel</span>.<span class=\"pl-s1\">build</span> <span class=\"pl-c1\">=</span> <span class=\"pl-k\">lambda</span> <span class=\"pl-c1\">*</span><span class=\"pl-s1\">_</span>: <span class=\"pl-c1\">None</span>\n<span class=\"pl-s1\">outer_output</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TimeDistributedModel</span>(<span class=\"pl-s1\">outer_in1</span>)\n<span class=\"pl-s1\">outer_output</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">SomeOtherComputations</span>()(<span class=\"pl-s1\">outer_output</span>)\n<span class=\"pl-s1\">outer_model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Model</span>(<span class=\"pl-s1\">input</span><span class=\"pl-c1\">=</span><span class=\"pl-s1\">outer_in1</span>, <span class=\"pl-s1\">output</span><span class=\"pl-c1\">=</span><span class=\"pl-s1\">outer_output</span>, <span class=\"pl-s1\">name</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">\"outer_model\"</span>)\n<span class=\"pl-s1\">outer_model</span>.<span class=\"pl-en\">compile</span>(<span class=\"pl-s\">\"adam\"</span>, <span class=\"pl-s\">\"mse\"</span>)\n<span class=\"pl-s1\">outer_model</span>.<span class=\"pl-en\">_make_predict_function</span>()</pre></div>", "createdAt": "2016-10-26T14:39:00Z", "publishedAt": "2016-10-26T14:39:00Z", "lastEditedAt": "2016-10-26T14:52:08Z", "updatedAt": "2016-10-26T14:52:08Z", "reactions": {"edges": []}, "bodyParsed": [{"number": 1252, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "@sjebbara There is no reason for you to provide the batch_size unless you are having a stateful RNN.", "sentSegmented": ["sjebbara", "there", "is", "no", "reason", "for", "you", "to", "provide", "the", "batch_size", "unless", "you", "are", "having", "a", "stateful", "rnn"], "sentSegmentedWithoutStops": ["sjebbara", "reason", "provide", "batch_size", "unless", "stateful", "rnn"], "sentSegmentedWithoutStopsStemmed": ["sjebbara", "reason", "provid", "batch_siz", "unless", "state", "rnn"]}, {"number": 1253, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "Both rnn based and reshape based TimeDistributed implementations are strictly mathematically equivalent.", "sentSegmented": ["both", "rnn", "based", "and", "reshape", "based", "timedistributed", "implementations", "are", "strictly", "mathematically", "equivalent"], "sentSegmentedWithoutStops": ["rnn", "based", "reshape", "based", "timedistributed", "implementations", "strictly", "mathematically", "equivalent"], "sentSegmentedWithoutStopsStemmed": ["rnn", "base", "reshap", "base", "timedistribut", "implement", "strictli", "mathemat", "equival"]}, {"number": 1254, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "(reshape based implementation being faster).", "sentSegmented": ["reshape", "based", "implementation", "being", "faster"], "sentSegmentedWithoutStops": ["reshape", "based", "implementation", "faster"], "sentSegmentedWithoutStopsStemmed": ["reshap", "base", "implement", "faster"]}, {"number": 1255, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "If you still want to specify batch_size, here you go:", "sentSegmented": ["if", "you", "still", "want", "to", "specify", "batch_size", "here", "you", "go"], "sentSegmentedWithoutStops": ["still", "want", "specify", "batch_size", "go"], "sentSegmentedWithoutStopsStemmed": ["still", "want", "specifi", "batch_siz", "go"]}, {"number": 1256, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "outer_in1 = Input(batch_shape=(batch_size, n_sequences, n_elements, element_size), name=\"outer_in1\")", "sentSegmented": ["outer_in1", "input", "batch_shape=", "batch_size", "n_sequences", "n_elements", "element_size", "name=", "outer_in1"], "sentSegmentedWithoutStops": ["outer_in1", "input", "batch_shape=", "batch_size", "n_sequences", "n_elements", "element_size", "name=", "outer_in1"], "sentSegmentedWithoutStopsStemmed": ["outer_in1", "input", "batch_shape=", "batch_siz", "n_sequenc", "n_element", "element_s", "name=", "outer_in1"]}, {"number": 1257, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "TimeDistributedModel = TimeDistributed(inner_model, name=\"distr\")", "sentSegmented": ["timedistributedmodel", "timedistributed", "inner_model", "name=", "distr"], "sentSegmentedWithoutStops": ["timedistributedmodel", "timedistributed", "inner_model", "name=", "distr"], "sentSegmentedWithoutStopsStemmed": ["timedistributedmodel", "timedistribut", "inner_model", "name=", "distr"]}, {"number": 1258, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "TimeDistributedModel.build((None,) + outer_in1._keras_shape[1:])", "sentSegmented": ["timedistributedmodel.build", "none", "outer_in1._keras_shape", "1"], "sentSegmentedWithoutStops": ["timedistributedmodel.build", "none", "outer_in1._keras_shape", "1"], "sentSegmentedWithoutStopsStemmed": ["timedistributedmodel.build", "none", "outer_in1._keras_shap", "1"]}, {"number": 1259, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "TimeDistributedModel.build = lambda *_: None", "sentSegmented": ["timedistributedmodel.build", "lambda", "_", "none"], "sentSegmentedWithoutStops": ["timedistributedmodel.build", "none"], "sentSegmentedWithoutStopsStemmed": ["timedistributedmodel.build", "none"]}, {"number": 1260, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "outer_output = TimeDistributedModel(outer_in1)", "sentSegmented": ["outer_output", "timedistributedmodel", "outer_in1"], "sentSegmentedWithoutStops": ["outer_output", "timedistributedmodel", "outer_in1"], "sentSegmentedWithoutStopsStemmed": ["outer_output", "timedistributedmodel", "outer_in1"]}, {"number": 1261, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "outer_output = SomeOtherComputations()(outer_output)", "sentSegmented": ["outer_output", "someothercomputations", "outer_output"], "sentSegmentedWithoutStops": ["outer_output", "someothercomputations", "outer_output"], "sentSegmentedWithoutStopsStemmed": ["outer_output", "someothercomput", "outer_output"]}, {"number": 1262, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "outer_model = Model(input=outer_in1, output=outer_output, name=\"outer_model\")", "sentSegmented": ["outer_model", "model", "input=outer_in1", "output=outer_output", "name=", "outer_model"], "sentSegmentedWithoutStops": ["outer_model", "model", "input=outer_in1", "output=outer_output", "name=", "outer_model"], "sentSegmentedWithoutStopsStemmed": ["outer_model", "model", "input=outer_in1", "output=outer_output", "name=", "outer_model"]}, {"number": 1263, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "outer_model.compile(\"adam\", \"mse\")", "sentSegmented": ["outer_model.compile", "adam", "mse"], "sentSegmentedWithoutStops": ["outer_model.compile", "adam", "mse"], "sentSegmentedWithoutStopsStemmed": ["outer_model.compil", "adam", "mse"]}, {"number": 1264, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "outer_model._make_predict_function()", "sentSegmented": ["outer_model._make_predict_function"], "sentSegmentedWithoutStops": ["outer_model._make_predict_function"], "sentSegmentedWithoutStopsStemmed": ["outer_model._make_predict_funct"]}]}}, {"node": {"id": "MDEyOklzc3VlQ29tbWVudDI1NjM3MTIyNA==", "author": {"login": "farizrahman4u"}, "body": "Similarly @eyaler, \nTo drop the exact number of nodes at every time step (when batch_size has to be provided becauses of stateful RNN):\n\n``` python\nmodel = Sequential()\nmodel.add(LSTM(10, batch_input_shape=(100, 20, 10), stateful=True, return_sequences=True))\n\ndropout = TimeDistributed(Dropout(0.5))\ndropout.build((None,) + model.output_shape[1:])\ndropout.build = lambda *_: None\n\nmodel.add(dropout)\n\nmodel.add(...)\nmodel.add(...)\n```\n", "bodyText": "Similarly @eyaler,\nTo drop the exact number of nodes at every time step (when batch_size has to be provided becauses of stateful RNN):\nmodel = Sequential()\nmodel.add(LSTM(10, batch_input_shape=(100, 20, 10), stateful=True, return_sequences=True))\n\ndropout = TimeDistributed(Dropout(0.5))\ndropout.build((None,) + model.output_shape[1:])\ndropout.build = lambda *_: None\n\nmodel.add(dropout)\n\nmodel.add(...)\nmodel.add(...)", "bodyHTML": "<p>Similarly <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/eyaler/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/eyaler\">@eyaler</a>,<br>\nTo drop the exact number of nodes at every time step (when batch_size has to be provided becauses of stateful RNN):</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Sequential</span>()\n<span class=\"pl-s1\">model</span>.<span class=\"pl-en\">add</span>(<span class=\"pl-v\">LSTM</span>(<span class=\"pl-c1\">10</span>, <span class=\"pl-s1\">batch_input_shape</span><span class=\"pl-c1\">=</span>(<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">20</span>, <span class=\"pl-c1\">10</span>), <span class=\"pl-s1\">stateful</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-s1\">return_sequences</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">True</span>))\n\n<span class=\"pl-s1\">dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TimeDistributed</span>(<span class=\"pl-v\">Dropout</span>(<span class=\"pl-c1\">0.5</span>))\n<span class=\"pl-s1\">dropout</span>.<span class=\"pl-en\">build</span>((<span class=\"pl-c1\">None</span>,) <span class=\"pl-c1\">+</span> <span class=\"pl-s1\">model</span>.<span class=\"pl-s1\">output_shape</span>[<span class=\"pl-c1\">1</span>:])\n<span class=\"pl-s1\">dropout</span>.<span class=\"pl-s1\">build</span> <span class=\"pl-c1\">=</span> <span class=\"pl-k\">lambda</span> <span class=\"pl-c1\">*</span><span class=\"pl-s1\">_</span>: <span class=\"pl-c1\">None</span>\n\n<span class=\"pl-s1\">model</span>.<span class=\"pl-en\">add</span>(<span class=\"pl-s1\">dropout</span>)\n\n<span class=\"pl-s1\">model</span>.<span class=\"pl-en\">add</span>(...)\n<span class=\"pl-s1\">model</span>.<span class=\"pl-en\">add</span>(...)</pre></div>", "createdAt": "2016-10-26T14:50:16Z", "publishedAt": "2016-10-26T14:50:16Z", "lastEditedAt": "2016-10-26T17:05:39Z", "updatedAt": "2016-10-26T17:05:39Z", "reactions": {"edges": []}, "bodyParsed": [{"number": 1265, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "Similarly @eyaler,", "sentSegmented": ["similarly", "eyaler"], "sentSegmentedWithoutStops": ["similarly", "eyaler"], "sentSegmentedWithoutStopsStemmed": ["similarli", "eyal"]}, {"number": 1266, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "To drop the exact number of nodes at every time step (when batch_size has to be provided becauses of stateful RNN):", "sentSegmented": ["to", "drop", "the", "exact", "number", "of", "nodes", "at", "every", "time", "step", "when", "batch_size", "has", "to", "be", "provided", "becauses", "of", "stateful", "rnn"], "sentSegmentedWithoutStops": ["drop", "exact", "number", "nodes", "every", "time", "step", "batch_size", "provided", "becauses", "stateful", "rnn"], "sentSegmentedWithoutStopsStemmed": ["drop", "exact", "number", "node", "everi", "time", "step", "batch_siz", "provid", "becaus", "state", "rnn"]}, {"number": 1267, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "model = Sequential()", "sentSegmented": ["model", "sequential"], "sentSegmentedWithoutStops": ["model", "sequential"], "sentSegmentedWithoutStopsStemmed": ["model", "sequenti"]}, {"number": 1268, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "model.add(LSTM(10, batch_input_shape=(100, 20, 10), stateful=True, return_sequences=True))", "sentSegmented": ["model.add", "lstm", "10", "batch_input_shape=", "100", "20", "10", "stateful=true", "return_sequences=true"], "sentSegmentedWithoutStops": ["model.add", "lstm", "10", "batch_input_shape=", "100", "20", "10", "stateful=true", "return_sequences=true"], "sentSegmentedWithoutStopsStemmed": ["model.add", "lstm", "10", "batch_input_shape=", "100", "20", "10", "stateful=tru", "return_sequences=tru"]}, {"number": 1269, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "dropout = TimeDistributed(Dropout(0.5))", "sentSegmented": ["dropout", "timedistributed", "dropout", "0.5"], "sentSegmentedWithoutStops": ["dropout", "timedistributed", "dropout", "0.5"], "sentSegmentedWithoutStopsStemmed": ["dropout", "timedistribut", "dropout", "0.5"]}, {"number": 1270, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "dropout.build((None,) + model.output_shape[1:])", "sentSegmented": ["dropout.build", "none", "model.output_shape", "1"], "sentSegmentedWithoutStops": ["dropout.build", "none", "model.output_shape", "1"], "sentSegmentedWithoutStopsStemmed": ["dropout.build", "none", "model.output_shap", "1"]}, {"number": 1271, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "dropout.build = lambda *_: None", "sentSegmented": ["dropout.build", "lambda", "_", "none"], "sentSegmentedWithoutStops": ["dropout.build", "none"], "sentSegmentedWithoutStopsStemmed": ["dropout.build", "none"]}, {"number": 1272, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "model.add(dropout)", "sentSegmented": ["model.add", "dropout"], "sentSegmentedWithoutStops": ["model.add", "dropout"], "sentSegmentedWithoutStopsStemmed": ["model.add", "dropout"]}, {"number": 1273, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "model.add(...)", "sentSegmented": ["model.add"], "sentSegmentedWithoutStops": ["model.add"], "sentSegmentedWithoutStopsStemmed": ["model.add"]}, {"number": 1274, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "model.add(...)", "sentSegmented": ["model.add"], "sentSegmentedWithoutStops": ["model.add"], "sentSegmentedWithoutStopsStemmed": ["model.add"]}]}}, {"node": {"id": "MDEyOklzc3VlQ29tbWVudDI1NjQxMDc4Mg==", "author": {"login": "eyaler"}, "body": "thanks @farizrahman4u !\n1. if reshape is faster why isn't it used also when batch_size is given?\n2. how would your solution look using the functional api? my attempt failed on assert_input_compatibility(x)\n", "bodyText": "thanks @farizrahman4u !\n\nif reshape is faster why isn't it used also when batch_size is given?\nhow would your solution look using the functional api? my attempt failed on assert_input_compatibility(x)", "bodyHTML": "<p>thanks <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/farizrahman4u/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/farizrahman4u\">@farizrahman4u</a> !</p>\n<ol>\n<li>if reshape is faster why isn't it used also when batch_size is given?</li>\n<li>how would your solution look using the functional api? my attempt failed on assert_input_compatibility(x)</li>\n</ol>", "createdAt": "2016-10-26T16:54:07Z", "publishedAt": "2016-10-26T16:54:07Z", "lastEditedAt": "2016-10-26T16:56:30Z", "updatedAt": "2016-10-26T16:56:30Z", "reactions": {"edges": []}, "bodyParsed": [{"number": 1275, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "thanks @farizrahman4u !", "sentSegmented": ["thanks", "farizrahman4u"], "sentSegmentedWithoutStops": ["thanks", "farizrahman4u"], "sentSegmentedWithoutStopsStemmed": ["thank", "farizrahman4u"]}, {"number": 1276, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "if reshape is faster why isn't it used also when batch_size is given?", "sentSegmented": ["if", "reshape", "is", "faster", "why", "is", "n't", "it", "used", "also", "when", "batch_size", "is", "given"], "sentSegmentedWithoutStops": ["reshape", "faster", "n't", "used", "also", "batch_size", "given"], "sentSegmentedWithoutStopsStemmed": ["reshap", "faster", "n't", "use", "also", "batch_siz", "given"]}, {"number": 1277, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "how would your solution look using the functional api?", "sentSegmented": ["how", "would", "your", "solution", "look", "using", "the", "functional", "api"], "sentSegmentedWithoutStops": ["would", "solution", "look", "using", "functional", "api"], "sentSegmentedWithoutStopsStemmed": ["would", "solut", "look", "use", "function", "api"]}, {"number": 1278, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "my attempt failed on assert_input_compatibility(x)", "sentSegmented": ["my", "attempt", "failed", "on", "assert_input_compatibility", "x"], "sentSegmentedWithoutStops": ["attempt", "failed", "assert_input_compatibility", "x"], "sentSegmentedWithoutStopsStemmed": ["attempt", "fail", "assert_input_compat", "x"]}]}}, {"node": {"id": "MDEyOklzc3VlQ29tbWVudDI1NjQxNDAxNQ==", "author": {"login": "farizrahman4u"}, "body": "1. If batch size is given, then it is possible that the layer being wrapped is a stateful RNN (or any layer which requires a static batch size). Since the reshape method messes with the batch dimension, we go for the rnn method instead\n2. Maybe you forgot return_sequences=True\n", "bodyText": "If batch size is given, then it is possible that the layer being wrapped is a stateful RNN (or any layer which requires a static batch size). Since the reshape method messes with the batch dimension, we go for the rnn method instead\nMaybe you forgot return_sequences=True", "bodyHTML": "<ol>\n<li>If batch size is given, then it is possible that the layer being wrapped is a stateful RNN (or any layer which requires a static batch size). Since the reshape method messes with the batch dimension, we go for the rnn method instead</li>\n<li>Maybe you forgot return_sequences=True</li>\n</ol>", "createdAt": "2016-10-26T17:06:02Z", "publishedAt": "2016-10-26T17:06:02Z", "lastEditedAt": "2016-10-26T17:06:11Z", "updatedAt": "2016-10-26T17:06:11Z", "reactions": {"edges": []}, "bodyParsed": [{"number": 1279, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "If batch size is given, then it is possible that the layer being wrapped is a stateful RNN (or any layer which requires a static batch size).", "sentSegmented": ["if", "batch", "size", "is", "given", "then", "it", "is", "possible", "that", "the", "layer", "being", "wrapped", "is", "a", "stateful", "rnn", "or", "any", "layer", "which", "requires", "a", "static", "batch", "size"], "sentSegmentedWithoutStops": ["batch", "size", "given", "possible", "layer", "wrapped", "stateful", "rnn", "layer", "requires", "static", "batch", "size"], "sentSegmentedWithoutStopsStemmed": ["batch", "size", "given", "possibl", "layer", "wrap", "state", "rnn", "layer", "requir", "static", "batch", "size"]}, {"number": 1280, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "Since the reshape method messes with the batch dimension, we go for the rnn method instead", "sentSegmented": ["since", "the", "reshape", "method", "messes", "with", "the", "batch", "dimension", "we", "go", "for", "the", "rnn", "method", "instead"], "sentSegmentedWithoutStops": ["since", "reshape", "method", "messes", "batch", "dimension", "go", "rnn", "method", "instead"], "sentSegmentedWithoutStopsStemmed": ["sinc", "reshap", "method", "mess", "batch", "dimens", "go", "rnn", "method", "instead"]}, {"number": 1281, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "Maybe you forgot return_sequences=True", "sentSegmented": ["maybe", "you", "forgot", "return_sequences=true"], "sentSegmentedWithoutStops": ["maybe", "forgot", "return_sequences=true"], "sentSegmentedWithoutStopsStemmed": ["mayb", "forgot", "return_sequences=tru"]}]}}, {"node": {"id": "MDEyOklzc3VlQ29tbWVudDI1NjQyNjM4Ng==", "author": {"login": "eyaler"}, "body": "got it!\n\nfrom keras.models import Sequential, Model\nfrom keras.layers import LSTM, Dropout, TimeDistributed, Input\nimport numpy as np\n\nx=np.zeros((100,20,10))\ny=np.zeros((100,20,10))\n\nmodel = Sequential()\nmodel.add(LSTM(10, batch_input_shape=(100, 20, 10), stateful=True, return_sequences=True))\ndropout = TimeDistributed(Dropout(0.5))\ndropout.build((None,) + model.output_shape[1:])\ndropout.build = lambda *_: None\nmodel.add(dropout)\nmodel.compile(optimizer='sgd', loss='mse')\nmodel.fit(x,y,nb_epoch=1,batch_size=100)\n\ninput = Input(batch_shape=(100, 20, 10))\na = LSTM(10, stateful=True, return_sequences=True)(input)\ndropout = TimeDistributed(Dropout(0.5))\ndropout.build((None,) + a._keras_shape[1:])\ndropout.build = lambda *_: None\noutput = dropout(a)\nfmodel = Model(input, output)\nfmodel.compile(optimizer='sgd', loss='mse')\nfmodel.fit(x,y,nb_epoch=1,batch_size=100)\n", "bodyText": "got it!\nfrom keras.models import Sequential, Model\nfrom keras.layers import LSTM, Dropout, TimeDistributed, Input\nimport numpy as np\nx=np.zeros((100,20,10))\ny=np.zeros((100,20,10))\nmodel = Sequential()\nmodel.add(LSTM(10, batch_input_shape=(100, 20, 10), stateful=True, return_sequences=True))\ndropout = TimeDistributed(Dropout(0.5))\ndropout.build((None,) + model.output_shape[1:])\ndropout.build = lambda *_: None\nmodel.add(dropout)\nmodel.compile(optimizer='sgd', loss='mse')\nmodel.fit(x,y,nb_epoch=1,batch_size=100)\ninput = Input(batch_shape=(100, 20, 10))\na = LSTM(10, stateful=True, return_sequences=True)(input)\ndropout = TimeDistributed(Dropout(0.5))\ndropout.build((None,) + a.keras_shape[1:])\ndropout.build = lambda *: None\noutput = dropout(a)\nfmodel = Model(input, output)\nfmodel.compile(optimizer='sgd', loss='mse')\nfmodel.fit(x,y,nb_epoch=1,batch_size=100)", "bodyHTML": "<p>got it!</p>\n<p>from keras.models import Sequential, Model<br>\nfrom keras.layers import LSTM, Dropout, TimeDistributed, Input<br>\nimport numpy as np</p>\n<p>x=np.zeros((100,20,10))<br>\ny=np.zeros((100,20,10))</p>\n<p>model = Sequential()<br>\nmodel.add(LSTM(10, batch_input_shape=(100, 20, 10), stateful=True, return_sequences=True))<br>\ndropout = TimeDistributed(Dropout(0.5))<br>\ndropout.build((None,) + model.output_shape[1:])<br>\ndropout.build = lambda *_: None<br>\nmodel.add(dropout)<br>\nmodel.compile(optimizer='sgd', loss='mse')<br>\nmodel.fit(x,y,nb_epoch=1,batch_size=100)</p>\n<p>input = Input(batch_shape=(100, 20, 10))<br>\na = LSTM(10, stateful=True, return_sequences=True)(input)<br>\ndropout = TimeDistributed(Dropout(0.5))<br>\ndropout.build((None,) + a.<em>keras_shape[1:])<br>\ndropout.build = lambda *</em>: None<br>\noutput = dropout(a)<br>\nfmodel = Model(input, output)<br>\nfmodel.compile(optimizer='sgd', loss='mse')<br>\nfmodel.fit(x,y,nb_epoch=1,batch_size=100)</p>", "createdAt": "2016-10-26T17:52:14Z", "publishedAt": "2016-10-26T17:52:14Z", "lastEditedAt": null, "updatedAt": "2016-10-26T17:52:14Z", "reactions": {"edges": []}, "bodyParsed": [{"number": 1282, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "got it!", "sentSegmented": ["got", "it"], "sentSegmentedWithoutStops": ["got"], "sentSegmentedWithoutStopsStemmed": ["got"]}, {"number": 1283, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "from keras.models import Sequential, Model", "sentSegmented": ["from", "keras.models", "import", "sequential", "model"], "sentSegmentedWithoutStops": ["keras.models", "import", "sequential", "model"], "sentSegmentedWithoutStopsStemmed": ["keras.model", "import", "sequenti", "model"]}, {"number": 1284, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "from keras.layers import LSTM, Dropout, TimeDistributed, Input", "sentSegmented": ["from", "keras.layers", "import", "lstm", "dropout", "timedistributed", "input"], "sentSegmentedWithoutStops": ["keras.layers", "import", "lstm", "dropout", "timedistributed", "input"], "sentSegmentedWithoutStopsStemmed": ["keras.lay", "import", "lstm", "dropout", "timedistribut", "input"]}, {"number": 1285, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "import numpy as np", "sentSegmented": ["import", "numpy", "as", "np"], "sentSegmentedWithoutStops": ["import", "numpy", "np"], "sentSegmentedWithoutStopsStemmed": ["import", "numpi", "np"]}, {"number": 1286, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "x=np.zeros((100,20,10))", "sentSegmented": ["x=np.zeros", "100,20,10"], "sentSegmentedWithoutStops": ["x=np.zeros", "100,20,10"], "sentSegmentedWithoutStopsStemmed": ["x=np.zero", "100,20,10"]}, {"number": 1287, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "y=np.zeros((100,20,10))", "sentSegmented": ["y=np.zeros", "100,20,10"], "sentSegmentedWithoutStops": ["y=np.zeros", "100,20,10"], "sentSegmentedWithoutStopsStemmed": ["y=np.zero", "100,20,10"]}, {"number": 1288, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "model = Sequential()", "sentSegmented": ["model", "sequential"], "sentSegmentedWithoutStops": ["model", "sequential"], "sentSegmentedWithoutStopsStemmed": ["model", "sequenti"]}, {"number": 1289, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "model.add(LSTM(10, batch_input_shape=(100, 20, 10), stateful=True, return_sequences=True))", "sentSegmented": ["model.add", "lstm", "10", "batch_input_shape=", "100", "20", "10", "stateful=true", "return_sequences=true"], "sentSegmentedWithoutStops": ["model.add", "lstm", "10", "batch_input_shape=", "100", "20", "10", "stateful=true", "return_sequences=true"], "sentSegmentedWithoutStopsStemmed": ["model.add", "lstm", "10", "batch_input_shape=", "100", "20", "10", "stateful=tru", "return_sequences=tru"]}, {"number": 1290, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "dropout = TimeDistributed(Dropout(0.5))", "sentSegmented": ["dropout", "timedistributed", "dropout", "0.5"], "sentSegmentedWithoutStops": ["dropout", "timedistributed", "dropout", "0.5"], "sentSegmentedWithoutStopsStemmed": ["dropout", "timedistribut", "dropout", "0.5"]}, {"number": 1291, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "dropout.build((None,) + model.output_shape[1:])", "sentSegmented": ["dropout.build", "none", "model.output_shape", "1"], "sentSegmentedWithoutStops": ["dropout.build", "none", "model.output_shape", "1"], "sentSegmentedWithoutStopsStemmed": ["dropout.build", "none", "model.output_shap", "1"]}, {"number": 1292, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "dropout.build = lambda *_: None", "sentSegmented": ["dropout.build", "lambda", "_", "none"], "sentSegmentedWithoutStops": ["dropout.build", "lambda", "none"], "sentSegmentedWithoutStopsStemmed": ["dropout.build", "lambda", "none"]}, {"number": 1293, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "model.add(dropout)", "sentSegmented": ["model.add", "dropout"], "sentSegmentedWithoutStops": ["model.add", "dropout"], "sentSegmentedWithoutStopsStemmed": ["model.add", "dropout"]}, {"number": 1294, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "model.compile(optimizer='sgd', loss='mse')", "sentSegmented": ["model.compile", "optimizer='sgd", "loss='mse"], "sentSegmentedWithoutStops": ["model.compile", "optimizer='sgd", "loss='mse"], "sentSegmentedWithoutStopsStemmed": ["model.compil", "optimizer='sgd", "loss='ms"]}, {"number": 1295, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "model.fit(x,y,nb_epoch=1,batch_size=100)", "sentSegmented": ["model.fit", "x", "y", "nb_epoch=1", "batch_size=100"], "sentSegmentedWithoutStops": ["model.fit", "x", "nb_epoch=1", "batch_size=100"], "sentSegmentedWithoutStopsStemmed": ["model.fit", "x", "nb_epoch=1", "batch_size=100"]}, {"number": 1296, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "input = Input(batch_shape=(100, 20, 10))", "sentSegmented": ["input", "input", "batch_shape=", "100", "20", "10"], "sentSegmentedWithoutStops": ["input", "input", "batch_shape=", "100", "20", "10"], "sentSegmentedWithoutStopsStemmed": ["input", "input", "batch_shape=", "100", "20", "10"]}, {"number": 1297, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "a = LSTM(10, stateful=True, return_sequences=True)(input)", "sentSegmented": ["a", "lstm", "10", "stateful=true", "return_sequences=true", "input"], "sentSegmentedWithoutStops": ["lstm", "10", "stateful=true", "return_sequences=true", "input"], "sentSegmentedWithoutStopsStemmed": ["lstm", "10", "stateful=tru", "return_sequences=tru", "input"]}, {"number": 1298, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "dropout = TimeDistributed(Dropout(0.5))", "sentSegmented": ["dropout", "timedistributed", "dropout", "0.5"], "sentSegmentedWithoutStops": ["dropout", "timedistributed", "dropout", "0.5"], "sentSegmentedWithoutStopsStemmed": ["dropout", "timedistribut", "dropout", "0.5"]}, {"number": 1299, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "dropout.build((None,) + a.keras_shape[1:])", "sentSegmented": ["dropout.build", "none", "a.keras_shape", "1"], "sentSegmentedWithoutStops": ["dropout.build", "none", "a.keras_shape", "1"], "sentSegmentedWithoutStopsStemmed": ["dropout.build", "none", "a.keras_shap", "1"]}, {"number": 1300, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "dropout.build = lambda *: None", "sentSegmented": ["dropout.build", "lambda", "none"], "sentSegmentedWithoutStops": ["dropout.build", "lambda", "none"], "sentSegmentedWithoutStopsStemmed": ["dropout.build", "lambda", "none"]}, {"number": 1301, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "output = dropout(a)", "sentSegmented": ["output", "dropout", "a"], "sentSegmentedWithoutStops": ["output", "dropout"], "sentSegmentedWithoutStopsStemmed": ["output", "dropout"]}, {"number": 1302, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "fmodel = Model(input, output)", "sentSegmented": ["fmodel", "model", "input", "output"], "sentSegmentedWithoutStops": ["fmodel", "model", "input", "output"], "sentSegmentedWithoutStopsStemmed": ["fmodel", "model", "input", "output"]}, {"number": 1303, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "fmodel.compile(optimizer='sgd', loss='mse')", "sentSegmented": ["fmodel.compile", "optimizer='sgd", "loss='mse"], "sentSegmentedWithoutStops": ["fmodel.compile", "optimizer='sgd", "loss='mse"], "sentSegmentedWithoutStopsStemmed": ["fmodel.compil", "optimizer='sgd", "loss='ms"]}, {"number": 1304, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "fmodel.fit(x,y,nb_epoch=1,batch_size=100)", "sentSegmented": ["fmodel.fit", "x", "y", "nb_epoch=1", "batch_size=100"], "sentSegmentedWithoutStops": ["fmodel.fit", "x", "nb_epoch=1", "batch_size=100"], "sentSegmentedWithoutStopsStemmed": ["fmodel.fit", "x", "nb_epoch=1", "batch_size=100"]}]}}, {"node": {"id": "MDEyOklzc3VlQ29tbWVudDI1NjQzNTc2OQ==", "author": {"login": "sjebbara"}, "body": "I think I misunderstood the reshape-based implementation. I was just about to point out why reshaping makes no sense with a distributed RNN layer, but then the pieces fell together \ud83d\ude06.\n\nSo the solution is simply to leave `batch_size` undefined?!\nI will try that tomorrow.\nThanks all!\n", "bodyText": "I think I misunderstood the reshape-based implementation. I was just about to point out why reshaping makes no sense with a distributed RNN layer, but then the pieces fell together \ud83d\ude06.\nSo the solution is simply to leave batch_size undefined?!\nI will try that tomorrow.\nThanks all!", "bodyHTML": "<p>I think I misunderstood the reshape-based implementation. I was just about to point out why reshaping makes no sense with a distributed RNN layer, but then the pieces fell together <g-emoji class=\"g-emoji\" alias=\"laughing\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f606.png\">\ud83d\ude06</g-emoji>.</p>\n<p>So the solution is simply to leave <code>batch_size</code> undefined?!<br>\nI will try that tomorrow.<br>\nThanks all!</p>", "createdAt": "2016-10-26T18:26:07Z", "publishedAt": "2016-10-26T18:26:07Z", "lastEditedAt": null, "updatedAt": "2016-10-26T18:26:07Z", "reactions": {"edges": []}, "bodyParsed": [{"number": 1305, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "I think I misunderstood the reshape-based implementation.", "sentSegmented": ["i", "think", "i", "misunderstood", "the", "reshape-based", "implementation"], "sentSegmentedWithoutStops": ["think", "misunderstood", "reshape-based", "implementation"], "sentSegmentedWithoutStopsStemmed": ["think", "misunderstood", "reshape-bas", "implement"]}, {"number": 1306, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "I was just about to point out why reshaping makes no sense with a distributed RNN layer, but then the pieces fell together \ud83d\ude06.", "sentSegmented": ["i", "was", "just", "about", "to", "point", "out", "why", "reshaping", "makes", "no", "sense", "with", "a", "distributed", "rnn", "layer", "but", "then", "the", "pieces", "fell", "together"], "sentSegmentedWithoutStops": ["point", "reshaping", "makes", "sense", "distributed", "rnn", "layer", "pieces", "fell", "together"], "sentSegmentedWithoutStopsStemmed": ["point", "reshap", "make", "sens", "distribut", "rnn", "layer", "piec", "fell", "togeth"]}, {"number": 1307, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "So the solution is simply to leave batch_size undefined?", "sentSegmented": ["so", "the", "solution", "is", "simply", "to", "leave", "batch_size", "undefined"], "sentSegmentedWithoutStops": ["solution", "simply", "leave", "batch_size", "undefined"], "sentSegmentedWithoutStopsStemmed": ["solut", "simpli", "leav", "batch_siz", "undefin"]}, {"number": 1308, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "!", "sentSegmented": [], "sentSegmentedWithoutStops": [], "sentSegmentedWithoutStopsStemmed": []}, {"number": 1309, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "I will try that tomorrow.", "sentSegmented": ["i", "will", "try", "that", "tomorrow"], "sentSegmentedWithoutStops": ["try", "tomorrow"], "sentSegmentedWithoutStopsStemmed": ["tri", "tomorrow"]}, {"number": 1310, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "Thanks all!", "sentSegmented": ["thanks", "all"], "sentSegmentedWithoutStops": ["thanks"], "sentSegmentedWithoutStopsStemmed": ["thank"]}]}}, {"node": {"id": "MDEyOklzc3VlQ29tbWVudDI5NTcxMjgyMg==", "author": {"login": "tati-"}, "body": "Hello!\r\n\r\nIs there any update on this?\r\nBy the way for me it works with a tensorflow backend, but not with the theano one... ", "bodyText": "Hello!\nIs there any update on this?\nBy the way for me it works with a tensorflow backend, but not with the theano one...", "bodyHTML": "<p>Hello!</p>\n<p>Is there any update on this?<br>\nBy the way for me it works with a tensorflow backend, but not with the theano one...</p>", "createdAt": "2017-04-20T12:16:01Z", "publishedAt": "2017-04-20T12:16:01Z", "lastEditedAt": null, "updatedAt": "2017-04-20T12:16:01Z", "reactions": {"edges": []}, "bodyParsed": [{"number": 1311, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "Hello!", "sentSegmented": ["hello"], "sentSegmentedWithoutStops": ["hello"], "sentSegmentedWithoutStopsStemmed": ["hello"]}, {"number": 1312, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "Is there any update on this?", "sentSegmented": ["is", "there", "any", "update", "on", "this"], "sentSegmentedWithoutStops": ["update"], "sentSegmentedWithoutStopsStemmed": ["updat"]}, {"number": 1313, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "By the way for me it works with a tensorflow backend, but not with the theano one...", "sentSegmented": ["by", "the", "way", "for", "me", "it", "works", "with", "a", "tensorflow", "backend", "but", "not", "with", "the", "theano", "one"], "sentSegmentedWithoutStops": ["way", "works", "tensorflow", "backend", "theano", "one"], "sentSegmentedWithoutStopsStemmed": ["way", "work", "tensorflow", "backend", "theano", "one"]}]}}, {"node": {"id": "MDEyOklzc3VlQ29tbWVudDMxNjM4MTA3Mg==", "author": {"login": "stale"}, "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n", "bodyText": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.", "bodyHTML": "<p>This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.</p>", "createdAt": "2017-07-19T13:10:28Z", "publishedAt": "2017-07-19T13:10:28Z", "lastEditedAt": null, "updatedAt": "2017-07-19T13:10:28Z", "reactions": {"edges": []}, "bodyParsed": [{"number": 1314, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "This issue has been automatically marked as stale because it has not had recent activity.", "sentSegmented": ["this", "issue", "has", "been", "automatically", "marked", "as", "stale", "because", "it", "has", "not", "had", "recent", "activity"], "sentSegmentedWithoutStops": ["issue", "automatically", "marked", "stale", "recent", "activity"], "sentSegmentedWithoutStopsStemmed": ["issu", "automat", "mark", "stale", "recent", "activ"]}, {"number": 1315, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.", "sentSegmented": ["it", "will", "be", "closed", "after", "30", "days", "if", "no", "further", "activity", "occurs", "but", "feel", "free", "to", "re-open", "a", "closed", "issue", "if", "needed"], "sentSegmentedWithoutStops": ["closed", "30", "days", "activity", "occurs", "feel", "free", "re-open", "closed", "issue", "needed"], "sentSegmentedWithoutStopsStemmed": ["close", "30", "day", "activ", "occur", "feel", "free", "re-open", "close", "issu", "need"]}]}}, {"node": {"id": "MDEyOklzc3VlQ29tbWVudDMzNDI3MzcwNw==", "author": {"login": "brayan07"}, "body": "I was having a similar issue with Tensorflow.  Whenever I used the TimeDistributed wrapper on a model containing layers that used the learning phase, the resulting tensor would have the property _uses_learning_phase = False.  This meant that when I created a final model containing that tensor, the model's _uses_learning_phase would incorrectly be set to False.\r\n\r\nIn the case below, my intermediate_model had a Dropout layer; before passing it through the wrapper, intermediate_model.uses_learning_phase=True.\r\n\r\n    input_scan = Input(shape=(ANGLES,FINAL_WIDTH,FINAL_HEIGHT//2,CHANNELS))\r\n    #Time distributed model\r\n    sequenced_model = TimeDistributed(intermediate_model)(input_scan)\r\n  \r\n    sequenced_model._uses_learning_phase = True #Manually setting the tensor's property fixed the issue.\r\n    \r\n    out = GlobalAveragePooling1D()(sequenced_model)\r\n    #Complete model\r\n    model = Model(input_scan,out)", "bodyText": "I was having a similar issue with Tensorflow.  Whenever I used the TimeDistributed wrapper on a model containing layers that used the learning phase, the resulting tensor would have the property _uses_learning_phase = False.  This meant that when I created a final model containing that tensor, the model's _uses_learning_phase would incorrectly be set to False.\nIn the case below, my intermediate_model had a Dropout layer; before passing it through the wrapper, intermediate_model.uses_learning_phase=True.\ninput_scan = Input(shape=(ANGLES,FINAL_WIDTH,FINAL_HEIGHT//2,CHANNELS))\n#Time distributed model\nsequenced_model = TimeDistributed(intermediate_model)(input_scan)\n\nsequenced_model._uses_learning_phase = True #Manually setting the tensor's property fixed the issue.\n\nout = GlobalAveragePooling1D()(sequenced_model)\n#Complete model\nmodel = Model(input_scan,out)", "bodyHTML": "<p>I was having a similar issue with Tensorflow.  Whenever I used the TimeDistributed wrapper on a model containing layers that used the learning phase, the resulting tensor would have the property _uses_learning_phase = False.  This meant that when I created a final model containing that tensor, the model's _uses_learning_phase would incorrectly be set to False.</p>\n<p>In the case below, my intermediate_model had a Dropout layer; before passing it through the wrapper, intermediate_model.uses_learning_phase=True.</p>\n<pre><code>input_scan = Input(shape=(ANGLES,FINAL_WIDTH,FINAL_HEIGHT//2,CHANNELS))\n#Time distributed model\nsequenced_model = TimeDistributed(intermediate_model)(input_scan)\n\nsequenced_model._uses_learning_phase = True #Manually setting the tensor's property fixed the issue.\n\nout = GlobalAveragePooling1D()(sequenced_model)\n#Complete model\nmodel = Model(input_scan,out)\n</code></pre>", "createdAt": "2017-10-04T20:06:30Z", "publishedAt": "2017-10-04T20:06:30Z", "lastEditedAt": "2017-10-04T20:07:37Z", "updatedAt": "2017-10-04T20:07:37Z", "reactions": {"edges": [{"node": {"content": "HEART", "createdAt": "2018-09-17T07:04:26Z"}}, {"node": {"content": "HEART", "createdAt": "2018-10-22T10:28:43Z"}}, {"node": {"content": "HEART", "createdAt": "2020-05-27T09:10:17Z"}}]}, "bodyParsed": [{"number": 1316, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "I was having a similar issue with Tensorflow.", "sentSegmented": ["i", "was", "having", "a", "similar", "issue", "with", "tensorflow"], "sentSegmentedWithoutStops": ["similar", "issue", "tensorflow"], "sentSegmentedWithoutStopsStemmed": ["similar", "issu", "tensorflow"]}, {"number": 1317, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "Whenever I used the TimeDistributed wrapper on a model containing layers that used the learning phase, the resulting tensor would have the property _uses_learning_phase = False.", "sentSegmented": ["whenever", "i", "used", "the", "timedistributed", "wrapper", "on", "a", "model", "containing", "layers", "that", "used", "the", "learning", "phase", "the", "resulting", "tensor", "would", "have", "the", "property", "_uses_learning_phase", "false"], "sentSegmentedWithoutStops": ["whenever", "used", "timedistributed", "wrapper", "model", "containing", "layers", "used", "learning", "phase", "resulting", "tensor", "would", "property", "_uses_learning_phase", "false"], "sentSegmentedWithoutStopsStemmed": ["whenev", "use", "timedistribut", "wrapper", "model", "contain", "layer", "use", "learn", "phase", "result", "tensor", "would", "properti", "_uses_learning_phas", "fals"]}, {"number": 1318, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "This meant that when I created a final model containing that tensor, the model's _uses_learning_phase would incorrectly be set to False.", "sentSegmented": ["this", "meant", "that", "when", "i", "created", "a", "final", "model", "containing", "that", "tensor", "the", "model", "'s", "_uses_learning_phase", "would", "incorrectly", "be", "set", "to", "false"], "sentSegmentedWithoutStops": ["meant", "created", "final", "model", "containing", "tensor", "model", "'s", "_uses_learning_phase", "would", "incorrectly", "set", "false"], "sentSegmentedWithoutStopsStemmed": ["meant", "creat", "final", "model", "contain", "tensor", "model", "'s", "_uses_learning_phas", "would", "incorrectli", "set", "fals"]}, {"number": 1319, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "In the case below, my intermediate_model had a Dropout layer; before passing it through the wrapper, intermediate_model.uses_learning_phase=True.", "sentSegmented": ["in", "the", "case", "below", "my", "intermediate_model", "had", "a", "dropout", "layer", "before", "passing", "it", "through", "the", "wrapper", "intermediate_model.uses_learning_phase=true"], "sentSegmentedWithoutStops": ["case", "intermediate_model", "dropout", "layer", "passing", "wrapper", "intermediate_model.uses_learning_phase=true"], "sentSegmentedWithoutStopsStemmed": ["case", "intermediate_model", "dropout", "layer", "pass", "wrapper", "intermediate_model.uses_learning_phase=tru"]}, {"number": 1320, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "input_scan = Input(shape=(ANGLES,FINAL_WIDTH,FINAL_HEIGHT//2,CHANNELS))", "sentSegmented": ["input_scan", "input", "shape=", "angles", "final_width", "final_height//2", "channels"], "sentSegmentedWithoutStops": ["input_scan", "input", "shape=", "angles", "final_width", "final_height//2", "channels"], "sentSegmentedWithoutStopsStemmed": ["input_scan", "input", "shape=", "angl", "final_width", "final_height//2", "channel"]}, {"number": 1321, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "#Time distributed model", "sentSegmented": ["time", "distributed", "model"], "sentSegmentedWithoutStops": ["distributed", "model"], "sentSegmentedWithoutStopsStemmed": ["distribut", "model"]}, {"number": 1322, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "sequenced_model = TimeDistributed(intermediate_model)(input_scan)", "sentSegmented": ["sequenced_model", "timedistributed", "intermediate_model", "input_scan"], "sentSegmentedWithoutStops": ["sequenced_model", "timedistributed", "intermediate_model", "input_scan"], "sentSegmentedWithoutStopsStemmed": ["sequenced_model", "timedistribut", "intermediate_model", "input_scan"]}, {"number": 1323, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "sequenced_model._uses_learning_phase = True #Manually setting the tensor's property fixed the issue.", "sentSegmented": ["sequenced_model._uses_learning_phase", "true", "manually", "setting", "the", "tensor", "'s", "property", "fixed", "the", "issue"], "sentSegmentedWithoutStops": ["sequenced_model._uses_learning_phase", "true", "manually", "setting", "tensor", "'s", "property", "fixed", "issue"], "sentSegmentedWithoutStopsStemmed": ["sequenced_model._uses_learning_phas", "true", "manual", "set", "tensor", "'s", "properti", "fix", "issu"]}, {"number": 1324, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "out = GlobalAveragePooling1D()(sequenced_model)", "sentSegmented": ["out", "globalaveragepooling1d", "sequenced_model"], "sentSegmentedWithoutStops": ["globalaveragepooling1d", "sequenced_model"], "sentSegmentedWithoutStopsStemmed": ["globalaveragepooling1d", "sequenced_model"]}, {"number": 1325, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "#Complete model", "sentSegmented": ["complete", "model"], "sentSegmentedWithoutStops": ["complete", "model"], "sentSegmentedWithoutStopsStemmed": ["complet", "model"]}, {"number": 1326, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "model = Model(input_scan,out)", "sentSegmented": ["model", "model", "input_scan", "out"], "sentSegmentedWithoutStops": ["model", "model", "input_scan"], "sentSegmentedWithoutStopsStemmed": ["model", "model", "input_scan"]}]}}, {"node": {"id": "MDEyOklzc3VlQ29tbWVudDM1NjYwMjcyMA==", "author": {"login": "Nimi42"}, "body": "@eyaler\r\nI can't get your functional example to work.\r\n\r\nI tried with a Dense Layer instead of an LSTM. I get an error that says.\r\nTensors don't have keras_shape.\r\n\r\n```python\r\n\r\ndropout.build((None,) + a.keras_shape[1:])\r\n```\r\n\r\nThe other thing I tried was to have a Dense Layer as input to a dropout layer\r\nwrapped by a timeDistributed layer.\r\n\r\n```python\r\ninput_1 = Input(batch_shape=(batch_size, seq_len, num_inputs))\r\n\r\nx1 = Dense(32, activation='tanh')(input_1)\r\nx1 = TimeDistributed(Dropout(0.5))(x1)\r\n```\r\nwhich ends with:\r\n\r\n```python\r\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'time_distributed_1/keras_learning_phase' with dtype bool\r\n\t [[Node: time_distributed_1/keras_learning_phase = Placeholder[dtype=DT_BOOL, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n```\r\n\r\nEither way will cause an exception.\r\n\r\nWhat I want to do is sequence to sequence learning and I'd like to do it with the functional API.\r\n\r\nThat would be a timeDistributed dense layer on top of a LSTM if I understood correctly and\r\nthat works.\r\n\r\nHaving dropout would be the icing on the cake though.\r\n\r\nLike @farizrahman4u said I'd like to drop the exact same number of nodes at every time step with\r\na stateful RNN.\r\n\r\nCan anybody provide a pointer on how to do this with the functional API. I can't figure out this\r\nbuild magic.\r\n\r\n______________________________________________________________________________________\r\n**EDIT!:**\r\n\r\nI tried using \r\n```python\r\ntuple(a.get_shape().as_list())[1:]\r\n```\r\n\r\nto make the snippet work.\r\n\r\n```python\r\nfrom keras.models import Sequential, Model\r\nfrom keras.layers import LSTM, Dropout, TimeDistributed, Input\r\nimport numpy as np\r\n\r\nx=np.zeros((100,20,10))\r\ny=np.zeros((100,20,10))\r\n\r\ninput = Input(batch_shape=(100, 20, 10))\r\na = LSTM(10, stateful=True, return_sequences=True)(input)\r\ndropout = TimeDistributed(Dropout(0.5))\r\ndropout.build((None,) + tuple(a.get_shape().as_list())[1:])\r\ndropout.build = lambda *_: None\r\noutput = dropout(a)\r\nfmodel = Model(input, output)\r\nfmodel.compile(optimizer='sgd', loss='mse')\r\nfmodel.fit(x,y,nb_epoch=1,batch_size=100)\r\n```\r\n\r\nAgain it terminates with an exception. This time in the training phase:\r\n\r\n```python\r\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'time_distributed_1/keras_learning_phase' with dtype bool\r\n\t [[Node: time_distributed_1/keras_learning_phase = Placeholder[dtype=DT_BOOL, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n```\r\n\r\n**EDIT!:**\r\n\r\nThanks @brayan07 \r\n\r\nyour workaround fixed the issue and it compiles. I don't know if the dropout is applied correctly\r\nthough.\r\n", "bodyText": "@eyaler\nI can't get your functional example to work.\nI tried with a Dense Layer instead of an LSTM. I get an error that says.\nTensors don't have keras_shape.\ndropout.build((None,) + a.keras_shape[1:])\nThe other thing I tried was to have a Dense Layer as input to a dropout layer\nwrapped by a timeDistributed layer.\ninput_1 = Input(batch_shape=(batch_size, seq_len, num_inputs))\n\nx1 = Dense(32, activation='tanh')(input_1)\nx1 = TimeDistributed(Dropout(0.5))(x1)\nwhich ends with:\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'time_distributed_1/keras_learning_phase' with dtype bool\n\t [[Node: time_distributed_1/keras_learning_phase = Placeholder[dtype=DT_BOOL, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\nEither way will cause an exception.\nWhat I want to do is sequence to sequence learning and I'd like to do it with the functional API.\nThat would be a timeDistributed dense layer on top of a LSTM if I understood correctly and\nthat works.\nHaving dropout would be the icing on the cake though.\nLike @farizrahman4u said I'd like to drop the exact same number of nodes at every time step with\na stateful RNN.\nCan anybody provide a pointer on how to do this with the functional API. I can't figure out this\nbuild magic.\n\nEDIT!:\nI tried using\ntuple(a.get_shape().as_list())[1:]\nto make the snippet work.\nfrom keras.models import Sequential, Model\nfrom keras.layers import LSTM, Dropout, TimeDistributed, Input\nimport numpy as np\n\nx=np.zeros((100,20,10))\ny=np.zeros((100,20,10))\n\ninput = Input(batch_shape=(100, 20, 10))\na = LSTM(10, stateful=True, return_sequences=True)(input)\ndropout = TimeDistributed(Dropout(0.5))\ndropout.build((None,) + tuple(a.get_shape().as_list())[1:])\ndropout.build = lambda *_: None\noutput = dropout(a)\nfmodel = Model(input, output)\nfmodel.compile(optimizer='sgd', loss='mse')\nfmodel.fit(x,y,nb_epoch=1,batch_size=100)\nAgain it terminates with an exception. This time in the training phase:\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'time_distributed_1/keras_learning_phase' with dtype bool\n\t [[Node: time_distributed_1/keras_learning_phase = Placeholder[dtype=DT_BOOL, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\nEDIT!:\nThanks @brayan07\nyour workaround fixed the issue and it compiles. I don't know if the dropout is applied correctly\nthough.", "bodyHTML": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/eyaler/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/eyaler\">@eyaler</a><br>\nI can't get your functional example to work.</p>\n<p>I tried with a Dense Layer instead of an LSTM. I get an error that says.<br>\nTensors don't have keras_shape.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-s1\">dropout</span>.<span class=\"pl-en\">build</span>((<span class=\"pl-c1\">None</span>,) <span class=\"pl-c1\">+</span> <span class=\"pl-s1\">a</span>.<span class=\"pl-s1\">keras_shape</span>[<span class=\"pl-c1\">1</span>:])</pre></div>\n<p>The other thing I tried was to have a Dense Layer as input to a dropout layer<br>\nwrapped by a timeDistributed layer.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-s1\">input_1</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Input</span>(<span class=\"pl-s1\">batch_shape</span><span class=\"pl-c1\">=</span>(<span class=\"pl-s1\">batch_size</span>, <span class=\"pl-s1\">seq_len</span>, <span class=\"pl-s1\">num_inputs</span>))\n\n<span class=\"pl-s1\">x1</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Dense</span>(<span class=\"pl-c1\">32</span>, <span class=\"pl-s1\">activation</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">'tanh'</span>)(<span class=\"pl-s1\">input_1</span>)\n<span class=\"pl-s1\">x1</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TimeDistributed</span>(<span class=\"pl-v\">Dropout</span>(<span class=\"pl-c1\">0.5</span>))(<span class=\"pl-s1\">x1</span>)</pre></div>\n<p>which ends with:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-v\">InvalidArgumentError</span> (<span class=\"pl-s1\">see</span> <span class=\"pl-s1\">above</span> <span class=\"pl-k\">for</span> <span class=\"pl-s1\">traceback</span>): <span class=\"pl-v\">You</span> <span class=\"pl-s1\">must</span> <span class=\"pl-s1\">feed</span> <span class=\"pl-s1\">a</span> <span class=\"pl-s1\">value</span> <span class=\"pl-k\">for</span> <span class=\"pl-s1\">placeholder</span> <span class=\"pl-s1\">tensor</span> <span class=\"pl-s\">'time_distributed_1/keras_learning_phase'</span> <span class=\"pl-k\">with</span> <span class=\"pl-s1\">dtype</span> <span class=\"pl-s1\">bool</span>\n\t [[<span class=\"pl-v\">Node</span>: <span class=\"pl-s1\">time_distributed_1</span><span class=\"pl-c1\">/</span><span class=\"pl-s1\">keras_learning_phase</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Placeholder</span>[<span class=\"pl-s1\">dtype</span><span class=\"pl-c1\">=</span><span class=\"pl-v\">DT_BOOL</span>, <span class=\"pl-s1\">shape</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">&lt;</span><span class=\"pl-s1\">unknown</span><span class=\"pl-c1\">&gt;</span>, <span class=\"pl-s1\">_device</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">\"/job:localhost/replica:0/task:0/cpu:0\"</span>]()]]</pre></div>\n<p>Either way will cause an exception.</p>\n<p>What I want to do is sequence to sequence learning and I'd like to do it with the functional API.</p>\n<p>That would be a timeDistributed dense layer on top of a LSTM if I understood correctly and<br>\nthat works.</p>\n<p>Having dropout would be the icing on the cake though.</p>\n<p>Like <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/farizrahman4u/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/farizrahman4u\">@farizrahman4u</a> said I'd like to drop the exact same number of nodes at every time step with<br>\na stateful RNN.</p>\n<p>Can anybody provide a pointer on how to do this with the functional API. I can't figure out this<br>\nbuild magic.</p>\n<hr>\n<p><strong>EDIT!:</strong></p>\n<p>I tried using</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-en\">tuple</span>(<span class=\"pl-s1\">a</span>.<span class=\"pl-en\">get_shape</span>().<span class=\"pl-en\">as_list</span>())[<span class=\"pl-c1\">1</span>:]</pre></div>\n<p>to make the snippet work.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> <span class=\"pl-s1\">keras</span>.<span class=\"pl-s1\">models</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">Sequential</span>, <span class=\"pl-v\">Model</span>\n<span class=\"pl-k\">from</span> <span class=\"pl-s1\">keras</span>.<span class=\"pl-s1\">layers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">LSTM</span>, <span class=\"pl-v\">Dropout</span>, <span class=\"pl-v\">TimeDistributed</span>, <span class=\"pl-v\">Input</span>\n<span class=\"pl-k\">import</span> <span class=\"pl-s1\">numpy</span> <span class=\"pl-k\">as</span> <span class=\"pl-s1\">np</span>\n\n<span class=\"pl-s1\">x</span><span class=\"pl-c1\">=</span><span class=\"pl-s1\">np</span>.<span class=\"pl-en\">zeros</span>((<span class=\"pl-c1\">100</span>,<span class=\"pl-c1\">20</span>,<span class=\"pl-c1\">10</span>))\n<span class=\"pl-s1\">y</span><span class=\"pl-c1\">=</span><span class=\"pl-s1\">np</span>.<span class=\"pl-en\">zeros</span>((<span class=\"pl-c1\">100</span>,<span class=\"pl-c1\">20</span>,<span class=\"pl-c1\">10</span>))\n\n<span class=\"pl-s1\">input</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Input</span>(<span class=\"pl-s1\">batch_shape</span><span class=\"pl-c1\">=</span>(<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">20</span>, <span class=\"pl-c1\">10</span>))\n<span class=\"pl-s1\">a</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">LSTM</span>(<span class=\"pl-c1\">10</span>, <span class=\"pl-s1\">stateful</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-s1\">return_sequences</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">True</span>)(<span class=\"pl-s1\">input</span>)\n<span class=\"pl-s1\">dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TimeDistributed</span>(<span class=\"pl-v\">Dropout</span>(<span class=\"pl-c1\">0.5</span>))\n<span class=\"pl-s1\">dropout</span>.<span class=\"pl-en\">build</span>((<span class=\"pl-c1\">None</span>,) <span class=\"pl-c1\">+</span> <span class=\"pl-en\">tuple</span>(<span class=\"pl-s1\">a</span>.<span class=\"pl-en\">get_shape</span>().<span class=\"pl-en\">as_list</span>())[<span class=\"pl-c1\">1</span>:])\n<span class=\"pl-s1\">dropout</span>.<span class=\"pl-s1\">build</span> <span class=\"pl-c1\">=</span> <span class=\"pl-k\">lambda</span> <span class=\"pl-c1\">*</span><span class=\"pl-s1\">_</span>: <span class=\"pl-c1\">None</span>\n<span class=\"pl-s1\">output</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">dropout</span>(<span class=\"pl-s1\">a</span>)\n<span class=\"pl-s1\">fmodel</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Model</span>(<span class=\"pl-s1\">input</span>, <span class=\"pl-s1\">output</span>)\n<span class=\"pl-s1\">fmodel</span>.<span class=\"pl-en\">compile</span>(<span class=\"pl-s1\">optimizer</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">'sgd'</span>, <span class=\"pl-s1\">loss</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">'mse'</span>)\n<span class=\"pl-s1\">fmodel</span>.<span class=\"pl-en\">fit</span>(<span class=\"pl-s1\">x</span>,<span class=\"pl-s1\">y</span>,<span class=\"pl-s1\">nb_epoch</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">1</span>,<span class=\"pl-s1\">batch_size</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">100</span>)</pre></div>\n<p>Again it terminates with an exception. This time in the training phase:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-v\">InvalidArgumentError</span> (<span class=\"pl-s1\">see</span> <span class=\"pl-s1\">above</span> <span class=\"pl-k\">for</span> <span class=\"pl-s1\">traceback</span>): <span class=\"pl-v\">You</span> <span class=\"pl-s1\">must</span> <span class=\"pl-s1\">feed</span> <span class=\"pl-s1\">a</span> <span class=\"pl-s1\">value</span> <span class=\"pl-k\">for</span> <span class=\"pl-s1\">placeholder</span> <span class=\"pl-s1\">tensor</span> <span class=\"pl-s\">'time_distributed_1/keras_learning_phase'</span> <span class=\"pl-k\">with</span> <span class=\"pl-s1\">dtype</span> <span class=\"pl-s1\">bool</span>\n\t [[<span class=\"pl-v\">Node</span>: <span class=\"pl-s1\">time_distributed_1</span><span class=\"pl-c1\">/</span><span class=\"pl-s1\">keras_learning_phase</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Placeholder</span>[<span class=\"pl-s1\">dtype</span><span class=\"pl-c1\">=</span><span class=\"pl-v\">DT_BOOL</span>, <span class=\"pl-s1\">shape</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">&lt;</span><span class=\"pl-s1\">unknown</span><span class=\"pl-c1\">&gt;</span>, <span class=\"pl-s1\">_device</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">\"/job:localhost/replica:0/task:0/cpu:0\"</span>]()]]</pre></div>\n<p><strong>EDIT!:</strong></p>\n<p>Thanks <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/brayan07/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/brayan07\">@brayan07</a></p>\n<p>your workaround fixed the issue and it compiles. I don't know if the dropout is applied correctly<br>\nthough.</p>", "createdAt": "2018-01-10T13:31:25Z", "publishedAt": "2018-01-10T13:31:25Z", "lastEditedAt": "2018-01-11T08:41:05Z", "updatedAt": "2018-01-11T08:41:05Z", "reactions": {"edges": []}, "bodyParsed": [{"number": 1327, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "@eyaler", "sentSegmented": ["eyaler"], "sentSegmentedWithoutStops": ["eyaler"], "sentSegmentedWithoutStopsStemmed": ["eyal"]}, {"number": 1328, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "I can't get your functional example to work.", "sentSegmented": ["i", "ca", "n't", "get", "your", "functional", "example", "to", "work"], "sentSegmentedWithoutStops": ["ca", "n't", "get", "functional", "example", "work"], "sentSegmentedWithoutStopsStemmed": ["ca", "n't", "get", "function", "exampl", "work"]}, {"number": 1329, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "I tried with a Dense Layer instead of an LSTM.", "sentSegmented": ["i", "tried", "with", "a", "dense", "layer", "instead", "of", "an", "lstm"], "sentSegmentedWithoutStops": ["tried", "dense", "layer", "instead", "lstm"], "sentSegmentedWithoutStopsStemmed": ["tri", "dens", "layer", "instead", "lstm"]}, {"number": 1330, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "I get an error that says.", "sentSegmented": ["i", "get", "an", "error", "that", "says"], "sentSegmentedWithoutStops": ["get", "error", "says"], "sentSegmentedWithoutStopsStemmed": ["get", "error", "say"]}, {"number": 1331, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "Tensors don't have keras_shape.", "sentSegmented": ["tensors", "do", "n't", "have", "keras_shape"], "sentSegmentedWithoutStops": ["tensors", "n't", "keras_shape"], "sentSegmentedWithoutStopsStemmed": ["tensor", "n't", "keras_shap"]}, {"number": 1332, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "dropout.build((None,) + a.keras_shape[1:])", "sentSegmented": ["dropout.build", "none", "a.keras_shape", "1"], "sentSegmentedWithoutStops": ["dropout.build", "none", "a.keras_shape", "1"], "sentSegmentedWithoutStopsStemmed": ["dropout.build", "none", "a.keras_shap", "1"]}, {"number": 1333, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "The other thing I tried was to have a Dense Layer as input to a dropout layer", "sentSegmented": ["the", "other", "thing", "i", "tried", "was", "to", "have", "a", "dense", "layer", "as", "input", "to", "a", "dropout", "layer"], "sentSegmentedWithoutStops": ["thing", "tried", "dense", "layer", "input", "dropout", "layer"], "sentSegmentedWithoutStopsStemmed": ["thing", "tri", "dens", "layer", "input", "dropout", "layer"]}, {"number": 1334, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "wrapped by a timeDistributed layer.", "sentSegmented": ["wrapped", "by", "a", "timedistributed", "layer"], "sentSegmentedWithoutStops": ["wrapped", "timedistributed", "layer"], "sentSegmentedWithoutStopsStemmed": ["wrap", "timedistribut", "layer"]}, {"number": 1335, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "input_1 = Input(batch_shape=(batch_size, seq_len, num_inputs))", "sentSegmented": ["input_1", "input", "batch_shape=", "batch_size", "seq_len", "num_inputs"], "sentSegmentedWithoutStops": ["input_1", "input", "batch_shape=", "batch_size", "seq_len", "num_inputs"], "sentSegmentedWithoutStopsStemmed": ["input_1", "input", "batch_shape=", "batch_siz", "seq_len", "num_input"]}, {"number": 1336, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "x1 = Dense(32, activation='tanh')(input_1)", "sentSegmented": ["x1", "dense", "32", "activation='tanh", "input_1"], "sentSegmentedWithoutStops": ["x1", "dense", "32", "activation='tanh", "input_1"], "sentSegmentedWithoutStopsStemmed": ["x1", "dens", "32", "activation='tanh", "input_1"]}, {"number": 1337, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "x1 = TimeDistributed(Dropout(0.5))(x1)", "sentSegmented": ["x1", "timedistributed", "dropout", "0.5", "x1"], "sentSegmentedWithoutStops": ["x1", "timedistributed", "dropout", "0.5", "x1"], "sentSegmentedWithoutStopsStemmed": ["x1", "timedistribut", "dropout", "0.5", "x1"]}, {"number": 1338, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "which ends with:", "sentSegmented": ["which", "ends", "with"], "sentSegmentedWithoutStops": ["ends"], "sentSegmentedWithoutStopsStemmed": ["end"]}, {"number": 1339, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'time_distributed_1/keras_learning_phase' with dtype bool", "sentSegmented": ["invalidargumenterror", "see", "above", "for", "traceback", "you", "must", "feed", "a", "value", "for", "placeholder", "tensor", "'time_distributed_1/keras_learning_phase", "with", "dtype", "bool"], "sentSegmentedWithoutStops": ["invalidargumenterror", "see", "traceback", "must", "feed", "value", "placeholder", "tensor", "'time_distributed_1/keras_learning_phase", "dtype", "bool"], "sentSegmentedWithoutStopsStemmed": ["invalidargumenterror", "see", "traceback", "must", "feed", "valu", "placehold", "tensor", "'time_distributed_1/keras_learning_phas", "dtype", "bool"]}, {"number": 1340, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "\t [[Node: time_distributed_1/keras_learning_phase = Placeholder[dtype=DT_BOOL, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]", "sentSegmented": ["node", "time_distributed_1/keras_learning_phase", "placeholder", "dtype=dt_bool", "shape=", "unknown", "_device=", "/job", "localhost/replica:0/task:0/cpu:0"], "sentSegmentedWithoutStops": ["node", "time_distributed_1/keras_learning_phase", "placeholder", "dtype=dt_bool", "shape=", "unknown", "_device=", "/job", "localhost/replica:0/task:0/cpu:0"], "sentSegmentedWithoutStopsStemmed": ["node", "time_distributed_1/keras_learning_phas", "placehold", "dtype=dt_bool", "shape=", "unknown", "_device=", "/job", "localhost/replica:0/task:0/cpu:0"]}, {"number": 1341, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "Either way will cause an exception.", "sentSegmented": ["either", "way", "will", "cause", "an", "exception"], "sentSegmentedWithoutStops": ["either", "way", "cause", "exception"], "sentSegmentedWithoutStopsStemmed": ["either", "way", "caus", "except"]}, {"number": 1342, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "What I want to do is sequence to sequence learning and I'd like to do it with the functional API.", "sentSegmented": ["what", "i", "want", "to", "do", "is", "sequence", "to", "sequence", "learning", "and", "i", "'d", "like", "to", "do", "it", "with", "the", "functional", "api"], "sentSegmentedWithoutStops": ["want", "sequence", "sequence", "learning", "'d", "like", "functional", "api"], "sentSegmentedWithoutStopsStemmed": ["want", "sequenc", "sequenc", "learn", "'d", "like", "function", "api"]}, {"number": 1343, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "That would be a timeDistributed dense layer on top of a LSTM if I understood correctly and", "sentSegmented": ["that", "would", "be", "a", "timedistributed", "dense", "layer", "on", "top", "of", "a", "lstm", "if", "i", "understood", "correctly", "and"], "sentSegmentedWithoutStops": ["would", "timedistributed", "dense", "layer", "top", "lstm", "understood", "correctly"], "sentSegmentedWithoutStopsStemmed": ["would", "timedistribut", "dens", "layer", "top", "lstm", "understood", "correctli"]}, {"number": 1344, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "that works.", "sentSegmented": ["that", "works"], "sentSegmentedWithoutStops": ["works"], "sentSegmentedWithoutStopsStemmed": ["work"]}, {"number": 1345, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "Having dropout would be the icing on the cake though.", "sentSegmented": ["having", "dropout", "would", "be", "the", "icing", "on", "the", "cake", "though"], "sentSegmentedWithoutStops": ["dropout", "would", "icing", "cake", "though"], "sentSegmentedWithoutStopsStemmed": ["dropout", "would", "ice", "cake", "though"]}, {"number": 1346, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "Like @farizrahman4u said I'd like to drop the exact same number of nodes at every time step with", "sentSegmented": ["like", "farizrahman4u", "said", "i", "'d", "like", "to", "drop", "the", "exact", "same", "number", "of", "nodes", "at", "every", "time", "step", "with"], "sentSegmentedWithoutStops": ["like", "farizrahman4u", "said", "'d", "like", "drop", "exact", "number", "nodes", "every", "time", "step"], "sentSegmentedWithoutStopsStemmed": ["like", "farizrahman4u", "said", "'d", "like", "drop", "exact", "number", "node", "everi", "time", "step"]}, {"number": 1347, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "a stateful RNN.", "sentSegmented": ["a", "stateful", "rnn"], "sentSegmentedWithoutStops": ["stateful", "rnn"], "sentSegmentedWithoutStopsStemmed": ["state", "rnn"]}, {"number": 1348, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "Can anybody provide a pointer on how to do this with the functional API.", "sentSegmented": ["can", "anybody", "provide", "a", "pointer", "on", "how", "to", "do", "this", "with", "the", "functional", "api"], "sentSegmentedWithoutStops": ["anybody", "provide", "pointer", "functional", "api"], "sentSegmentedWithoutStopsStemmed": ["anybodi", "provid", "pointer", "function", "api"]}, {"number": 1349, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "I can't figure out this", "sentSegmented": ["i", "ca", "n't", "figure", "out", "this"], "sentSegmentedWithoutStops": ["ca", "n't", "figure"], "sentSegmentedWithoutStopsStemmed": ["ca", "n't", "figur"]}, {"number": 1350, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "build magic.", "sentSegmented": ["build", "magic"], "sentSegmentedWithoutStops": ["build", "magic"], "sentSegmentedWithoutStopsStemmed": ["build", "magic"]}, {"number": 1351, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "EDIT!", "sentSegmented": ["edit"], "sentSegmentedWithoutStops": ["edit"], "sentSegmentedWithoutStopsStemmed": ["edit"]}, {"number": 1352, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": ":", "sentSegmented": [], "sentSegmentedWithoutStops": [], "sentSegmentedWithoutStopsStemmed": []}, {"number": 1353, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "I tried using", "sentSegmented": ["i", "tried", "using"], "sentSegmentedWithoutStops": ["tried", "using"], "sentSegmentedWithoutStopsStemmed": ["tri", "use"]}, {"number": 1354, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "tuple(a.get_shape().as_list())[1:]", "sentSegmented": ["tuple", "a.get_shape", ".as_list", "1"], "sentSegmentedWithoutStops": ["tuple", "a.get_shape", ".as_list", "1"], "sentSegmentedWithoutStopsStemmed": ["tupl", "a.get_shap", ".as_list", "1"]}, {"number": 1355, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "to make the snippet work.", "sentSegmented": ["to", "make", "the", "snippet", "work"], "sentSegmentedWithoutStops": ["make", "snippet", "work"], "sentSegmentedWithoutStopsStemmed": ["make", "snippet", "work"]}, {"number": 1356, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "from keras.models import Sequential, Model", "sentSegmented": ["from", "keras.models", "import", "sequential", "model"], "sentSegmentedWithoutStops": ["keras.models", "sequential", "model"], "sentSegmentedWithoutStopsStemmed": ["keras.model", "sequenti", "model"]}, {"number": 1357, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "from keras.layers import LSTM, Dropout, TimeDistributed, Input", "sentSegmented": ["from", "keras.layers", "import", "lstm", "dropout", "timedistributed", "input"], "sentSegmentedWithoutStops": ["keras.layers", "lstm", "dropout", "timedistributed", "input"], "sentSegmentedWithoutStopsStemmed": ["keras.lay", "lstm", "dropout", "timedistribut", "input"]}, {"number": 1358, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "import numpy as np", "sentSegmented": ["import", "numpy", "as", "np"], "sentSegmentedWithoutStops": ["numpy", "np"], "sentSegmentedWithoutStopsStemmed": ["numpi", "np"]}, {"number": 1359, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "x=np.zeros((100,20,10))", "sentSegmented": ["x=np.zeros", "100,20,10"], "sentSegmentedWithoutStops": ["x=np.zeros", "100,20,10"], "sentSegmentedWithoutStopsStemmed": ["x=np.zero", "100,20,10"]}, {"number": 1360, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "y=np.zeros((100,20,10))", "sentSegmented": ["y=np.zeros", "100,20,10"], "sentSegmentedWithoutStops": ["y=np.zeros", "100,20,10"], "sentSegmentedWithoutStopsStemmed": ["y=np.zero", "100,20,10"]}, {"number": 1361, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "input = Input(batch_shape=(100, 20, 10))", "sentSegmented": ["input", "input", "batch_shape=", "100", "20", "10"], "sentSegmentedWithoutStops": ["input", "input", "batch_shape=", "100", "20", "10"], "sentSegmentedWithoutStopsStemmed": ["input", "input", "batch_shape=", "100", "20", "10"]}, {"number": 1362, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "a = LSTM(10, stateful=True, return_sequences=True)(input)", "sentSegmented": ["a", "lstm", "10", "stateful=true", "return_sequences=true", "input"], "sentSegmentedWithoutStops": ["lstm", "10", "stateful=true", "return_sequences=true", "input"], "sentSegmentedWithoutStopsStemmed": ["lstm", "10", "stateful=tru", "return_sequences=tru", "input"]}, {"number": 1363, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "dropout = TimeDistributed(Dropout(0.5))", "sentSegmented": ["dropout", "timedistributed", "dropout", "0.5"], "sentSegmentedWithoutStops": ["dropout", "timedistributed", "dropout", "0.5"], "sentSegmentedWithoutStopsStemmed": ["dropout", "timedistribut", "dropout", "0.5"]}, {"number": 1364, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "dropout.build((None,) + tuple(a.get_shape().as_list())[1:])", "sentSegmented": ["dropout.build", "none", "tuple", "a.get_shape", ".as_list", "1"], "sentSegmentedWithoutStops": ["dropout.build", "none", "tuple", "a.get_shape", ".as_list", "1"], "sentSegmentedWithoutStopsStemmed": ["dropout.build", "none", "tupl", "a.get_shap", ".as_list", "1"]}, {"number": 1365, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "dropout.build = lambda *_: None", "sentSegmented": ["dropout.build", "lambda", "_", "none"], "sentSegmentedWithoutStops": ["dropout.build", "none"], "sentSegmentedWithoutStopsStemmed": ["dropout.build", "none"]}, {"number": 1366, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "output = dropout(a)", "sentSegmented": ["output", "dropout", "a"], "sentSegmentedWithoutStops": ["output", "dropout"], "sentSegmentedWithoutStopsStemmed": ["output", "dropout"]}, {"number": 1367, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "fmodel = Model(input, output)", "sentSegmented": ["fmodel", "model", "input", "output"], "sentSegmentedWithoutStops": ["fmodel", "model", "input", "output"], "sentSegmentedWithoutStopsStemmed": ["fmodel", "model", "input", "output"]}, {"number": 1368, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "fmodel.compile(optimizer='sgd', loss='mse')", "sentSegmented": ["fmodel.compile", "optimizer='sgd", "loss='mse"], "sentSegmentedWithoutStops": ["fmodel.compile", "optimizer='sgd", "loss='mse"], "sentSegmentedWithoutStopsStemmed": ["fmodel.compil", "optimizer='sgd", "loss='ms"]}, {"number": 1369, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "fmodel.fit(x,y,nb_epoch=1,batch_size=100)", "sentSegmented": ["fmodel.fit", "x", "y", "nb_epoch=1", "batch_size=100"], "sentSegmentedWithoutStops": ["fmodel.fit", "x", "nb_epoch=1", "batch_size=100"], "sentSegmentedWithoutStopsStemmed": ["fmodel.fit", "x", "nb_epoch=1", "batch_size=100"]}, {"number": 1370, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "Again it terminates with an exception.", "sentSegmented": ["again", "it", "terminates", "with", "an", "exception"], "sentSegmentedWithoutStops": ["terminates", "exception"], "sentSegmentedWithoutStopsStemmed": ["termin", "except"]}, {"number": 1371, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "This time in the training phase:", "sentSegmented": ["this", "time", "in", "the", "training", "phase"], "sentSegmentedWithoutStops": ["time", "training", "phase"], "sentSegmentedWithoutStopsStemmed": ["time", "train", "phase"]}, {"number": 1372, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'time_distributed_1/keras_learning_phase' with dtype bool", "sentSegmented": ["invalidargumenterror", "see", "above", "for", "traceback", "you", "must", "feed", "a", "value", "for", "placeholder", "tensor", "'time_distributed_1/keras_learning_phase", "with", "dtype", "bool"], "sentSegmentedWithoutStops": ["invalidargumenterror", "see", "traceback", "must", "feed", "value", "placeholder", "tensor", "'time_distributed_1/keras_learning_phase", "dtype", "bool"], "sentSegmentedWithoutStopsStemmed": ["invalidargumenterror", "see", "traceback", "must", "feed", "valu", "placehold", "tensor", "'time_distributed_1/keras_learning_phas", "dtype", "bool"]}, {"number": 1373, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "\t [[Node: time_distributed_1/keras_learning_phase = Placeholder[dtype=DT_BOOL, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]", "sentSegmented": ["node", "time_distributed_1/keras_learning_phase", "placeholder", "dtype=dt_bool", "shape=", "unknown", "_device=", "/job", "localhost/replica:0/task:0/cpu:0"], "sentSegmentedWithoutStops": ["node", "time_distributed_1/keras_learning_phase", "placeholder", "dtype=dt_bool", "shape=", "unknown", "_device=", "/job", "localhost/replica:0/task:0/cpu:0"], "sentSegmentedWithoutStopsStemmed": ["node", "time_distributed_1/keras_learning_phas", "placehold", "dtype=dt_bool", "shape=", "unknown", "_device=", "/job", "localhost/replica:0/task:0/cpu:0"]}, {"number": 1374, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "EDIT!", "sentSegmented": ["edit"], "sentSegmentedWithoutStops": ["edit"], "sentSegmentedWithoutStopsStemmed": ["edit"]}, {"number": 1375, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": ":", "sentSegmented": [], "sentSegmentedWithoutStops": [], "sentSegmentedWithoutStopsStemmed": []}, {"number": 1376, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "Thanks @brayan07", "sentSegmented": ["thanks", "brayan07"], "sentSegmentedWithoutStops": ["thanks", "brayan07"], "sentSegmentedWithoutStopsStemmed": ["thank", "brayan07"]}, {"number": 1377, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "your workaround fixed the issue and it compiles.", "sentSegmented": ["your", "workaround", "fixed", "the", "issue", "and", "it", "compiles"], "sentSegmentedWithoutStops": ["workaround", "fixed", "issue", "compiles"], "sentSegmentedWithoutStopsStemmed": ["workaround", "fix", "issu", "compil"]}, {"number": 1378, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "I don't know if the dropout is applied correctly", "sentSegmented": ["i", "do", "n't", "know", "if", "the", "dropout", "is", "applied", "correctly"], "sentSegmentedWithoutStops": ["n't", "know", "dropout", "applied", "correctly"], "sentSegmentedWithoutStopsStemmed": ["n't", "know", "dropout", "appli", "correctli"]}, {"number": 1379, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "though.", "sentSegmented": ["though"], "sentSegmentedWithoutStops": ["though"], "sentSegmentedWithoutStopsStemmed": ["though"]}]}}, {"node": {"id": "MDEyOklzc3VlQ29tbWVudDQyMTkwOTYyNg==", "author": {"login": "davideboschetto"}, "body": "> sequenced_model._uses_learning_phase = True #Manually setting the tensor's property fixed the issue.\r\n> \r\n> ```\r\n\r\nThis was the key to solve this for me, too. \r\nThe model contained in the timedistributed was indeed not training without this.", "bodyText": "sequenced_model._uses_learning_phase = True #Manually setting the tensor's property fixed the issue.\n\n\nThis was the key to solve this for me, too.\nThe model contained in the timedistributed was indeed not training without this.", "bodyHTML": "<blockquote>\n<p>sequenced_model._uses_learning_phase = True #Manually setting the tensor's property fixed the issue.</p>\n<pre><code></code></pre>\n</blockquote>\n<p>This was the key to solve this for me, too.<br>\nThe model contained in the timedistributed was indeed not training without this.</p>", "createdAt": "2018-09-17T07:05:53Z", "publishedAt": "2018-09-17T07:05:53Z", "lastEditedAt": null, "updatedAt": "2018-09-17T07:05:53Z", "reactions": {"edges": []}, "bodyParsed": [{"number": 1380, "isCode": false, "isBlockQuote": true, "blockQuoteDepth": 1, "sent": "sequenced_model._uses_learning_phase = True #Manually setting the tensor's property fixed the issue.", "sentSegmented": ["sequenced_model._uses_learning_phase", "true", "manually", "setting", "the", "tensor", "'s", "property", "fixed", "the", "issue"], "sentSegmentedWithoutStops": ["sequenced_model._uses_learning_phase", "true", "manually", "setting", "tensor", "'s", "property", "fixed", "issue"], "sentSegmentedWithoutStopsStemmed": ["sequenced_model._uses_learning_phas", "true", "manual", "set", "tensor", "'s", "properti", "fix", "issu"]}, {"number": 1381, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "This was the key to solve this for me, too.", "sentSegmented": ["this", "was", "the", "key", "to", "solve", "this", "for", "me", "too"], "sentSegmentedWithoutStops": ["key", "solve"], "sentSegmentedWithoutStopsStemmed": ["key", "solv"]}, {"number": 1382, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "The model contained in the timedistributed was indeed not training without this.", "sentSegmented": ["the", "model", "contained", "in", "the", "timedistributed", "was", "indeed", "not", "training", "without", "this"], "sentSegmentedWithoutStops": ["model", "contained", "timedistributed", "indeed", "training", "without"], "sentSegmentedWithoutStopsStemmed": ["model", "contain", "timedistribut", "inde", "train", "without"]}]}}], "pageInfo": {"endCursor": "Y3Vyc29yOnYyOpHOGSXUeg==", "hasNextPage": false}, "totalCount": 16}, "labels": {"edges": [{"node": {"createdAt": "2017-05-23T18:28:56Z", "name": "stale"}}]}, "milestone": null, "reactions": {"edges": []}, "state": "CLOSED", "titleSegmented": ["problem", "with", "timedistributed", "and", "learning", "phase"], "titleSegmentedWithoutStops": ["problem", "timedistributed", "learning", "phase"], "titleSegmentedWithoutStopsStemmed": ["problem", "timedistribut", "learn", "phase"], "bodyParsed": [{"number": 1152, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "(EDIT: The following issue is only a minimal example of how to produce the error.", "sentSegmented": ["edit", "the", "following", "issue", "is", "only", "a", "minimal", "example", "of", "how", "to", "produce", "the", "error"], "sentSegmentedWithoutStops": ["edit", "following", "issue", "minimal", "example", "produce", "error"], "sentSegmentedWithoutStopsStemmed": ["edit", "follow", "issu", "minim", "exampl", "produc", "error"]}, {"number": 1153, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "My actual goal is to use a more complicated model instead of Dropout() here.)", "sentSegmented": ["my", "actual", "goal", "is", "to", "use", "a", "more", "complicated", "model", "instead", "of", "dropout", "here"], "sentSegmentedWithoutStops": ["actual", "goal", "use", "complicated", "model", "instead", "dropout"], "sentSegmentedWithoutStopsStemmed": ["actual", "goal", "use", "complic", "model", "instead", "dropout"]}, {"number": 1154, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "When executing the following script a MissingInputError occurs:", "sentSegmented": ["when", "executing", "the", "following", "script", "a", "missinginputerror", "occurs"], "sentSegmentedWithoutStops": ["executing", "following", "script", "missinginputerror", "occurs"], "sentSegmentedWithoutStopsStemmed": ["execut", "follow", "script", "missinginputerror", "occur"]}, {"number": 1155, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "from keras.models import Model", "sentSegmented": ["from", "keras.models", "import", "model"], "sentSegmentedWithoutStops": ["keras.models", "model"], "sentSegmentedWithoutStopsStemmed": ["keras.model", "model"]}, {"number": 1156, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "from keras.layers import Input, TimeDistributed, Dropout", "sentSegmented": ["from", "keras.layers", "import", "input", "timedistributed", "dropout"], "sentSegmentedWithoutStops": ["keras.layers", "input", "timedistributed", "dropout"], "sentSegmentedWithoutStopsStemmed": ["keras.lay", "input", "timedistribut", "dropout"]}, {"number": 1157, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "in1 = Input(batch_shape=(10, 8, 6), name=\"in1\")", "sentSegmented": ["in1", "input", "batch_shape=", "10", "8", "6", "name=", "in1"], "sentSegmentedWithoutStops": ["in1", "input", "batch_shape=", "10", "8", "6", "name=", "in1"], "sentSegmentedWithoutStopsStemmed": ["in1", "input", "batch_shape=", "10", "8", "6", "name=", "in1"]}, {"number": 1158, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "out1 = TimeDistributed(Dropout(0.5))(in1)", "sentSegmented": ["out1", "timedistributed", "dropout", "0.5", "in1"], "sentSegmentedWithoutStops": ["out1", "timedistributed", "dropout", "0.5", "in1"], "sentSegmentedWithoutStopsStemmed": ["out1", "timedistribut", "dropout", "0.5", "in1"]}, {"number": 1159, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "model = Model(input=in1, output=out1)", "sentSegmented": ["model", "model", "input=in1", "output=out1"], "sentSegmentedWithoutStops": ["model", "model", "input=in1", "output=out1"], "sentSegmentedWithoutStopsStemmed": ["model", "model", "input=in1", "output=out1"]}, {"number": 1160, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "model.compile(\"adam\", \"mse\")", "sentSegmented": ["model.compile", "adam", "mse"], "sentSegmentedWithoutStops": ["model.compile", "adam", "mse"], "sentSegmentedWithoutStopsStemmed": ["model.compil", "adam", "mse"]}, {"number": 1161, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "model._make_predict_function()", "sentSegmented": ["model._make_predict_function"], "sentSegmentedWithoutStops": ["model._make_predict_function"], "sentSegmentedWithoutStopsStemmed": ["model._make_predict_funct"]}, {"number": 1162, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "This is the simplest model that produces the error (In my original architecture, I tried to distribute a more complex model).", "sentSegmented": ["this", "is", "the", "simplest", "model", "that", "produces", "the", "error", "in", "my", "original", "architecture", "i", "tried", "to", "distribute", "a", "more", "complex", "model"], "sentSegmentedWithoutStops": ["simplest", "model", "produces", "error", "original", "architecture", "tried", "distribute", "complex", "model"], "sentSegmentedWithoutStopsStemmed": ["simplest", "model", "produc", "error", "origin", "architectur", "tri", "distribut", "complex", "model"]}, {"number": 1163, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "The same issue occurs when replacing the Dropout() layer with e.g.", "sentSegmented": ["the", "same", "issue", "occurs", "when", "replacing", "the", "dropout", "layer", "with", "e.g"], "sentSegmentedWithoutStops": ["issue", "occurs", "replacing", "dropout", "layer", "e.g"], "sentSegmentedWithoutStopsStemmed": ["issu", "occur", "replac", "dropout", "layer", "e.g"]}, {"number": 1164, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "GaussianNoise(), GRU(dropout_W=0.5), but not for e.g.", "sentSegmented": ["gaussiannoise", "gru", "dropout_w=0.5", "but", "not", "for", "e.g"], "sentSegmentedWithoutStops": ["gaussiannoise", "gru", "dropout_w=0.5", "e.g"], "sentSegmentedWithoutStopsStemmed": ["gaussiannois", "gru", "dropout_w=0.5", "e.g"]}, {"number": 1165, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "Dense().", "sentSegmented": ["dense"], "sentSegmentedWithoutStops": ["dense"], "sentSegmentedWithoutStopsStemmed": ["dens"]}, {"number": 1166, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "I think the error boils down to the combination of TimeDistributed() and any layer (or model) that uses the learning phase.", "sentSegmented": ["i", "think", "the", "error", "boils", "down", "to", "the", "combination", "of", "timedistributed", "and", "any", "layer", "or", "model", "that", "uses", "the", "learning", "phase"], "sentSegmentedWithoutStops": ["think", "error", "boils", "combination", "timedistributed", "layer", "model", "uses", "learning", "phase"], "sentSegmentedWithoutStopsStemmed": ["think", "error", "boil", "combin", "timedistribut", "layer", "model", "use", "learn", "phase"]}, {"number": 1167, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "Maybe there is a conceptual problem with TimeDistributed() and the learning phase input?", "sentSegmented": ["maybe", "there", "is", "a", "conceptual", "problem", "with", "timedistributed", "and", "the", "learning", "phase", "input"], "sentSegmentedWithoutStops": ["maybe", "conceptual", "problem", "timedistributed", "learning", "phase", "input"], "sentSegmentedWithoutStopsStemmed": ["mayb", "conceptu", "problem", "timedistribut", "learn", "phase", "input"]}, {"number": 1168, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "These issues seem to be somewhat related: #3834, #2609, #3686, #2391", "sentSegmented": ["these", "issues", "seem", "to", "be", "somewhat", "related", "3834", "2609", "3686", "2391"], "sentSegmentedWithoutStops": ["issues", "seem", "somewhat", "related", "3834", "2609", "3686", "2391"], "sentSegmentedWithoutStopsStemmed": ["issu", "seem", "somewhat", "relat", "3834", "2609", "3686", "2391"]}, {"number": 1169, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "The full stack trace is this:", "sentSegmented": ["the", "full", "stack", "trace", "is", "this"], "sentSegmentedWithoutStops": ["full", "stack", "trace"], "sentSegmentedWithoutStopsStemmed": ["full", "stack", "trace"]}, {"number": 1170, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "... ", "sentSegmented": [], "sentSegmentedWithoutStops": [], "sentSegmentedWithoutStopsStemmed": []}, {"number": 1171, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "  File \"/homes/sjebbara/git/keras-original/keras/engine/training.py\", line 752, in _make_predict_function", "sentSegmented": ["file", "/homes/sjebbara/git/keras-original/keras/engine/training.py", "line", "752", "in", "_make_predict_function"], "sentSegmentedWithoutStops": ["file", "/homes/sjebbara/git/keras-original/keras/engine/training.py", "line", "752", "_make_predict_function"], "sentSegmentedWithoutStopsStemmed": ["file", "/homes/sjebbara/git/keras-original/keras/engine/training.pi", "line", "752", "_make_predict_funct"]}, {"number": 1172, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "    **kwargs)", "sentSegmented": ["kwargs"], "sentSegmentedWithoutStops": ["kwargs"], "sentSegmentedWithoutStopsStemmed": ["kwarg"]}, {"number": 1173, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "  File \"/homes/sjebbara/git/keras-original/keras/backend/theano_backend.py\", line 787, in function", "sentSegmented": ["file", "/homes/sjebbara/git/keras-original/keras/backend/theano_backend.py", "line", "787", "in", "function"], "sentSegmentedWithoutStops": ["file", "/homes/sjebbara/git/keras-original/keras/backend/theano_backend.py", "line", "787"], "sentSegmentedWithoutStopsStemmed": ["file", "/homes/sjebbara/git/keras-original/keras/backend/theano_backend.pi", "line", "787"]}, {"number": 1174, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "    return Function(inputs, outputs, updates=updates, **kwargs)", "sentSegmented": ["return", "function", "inputs", "outputs", "updates=updates", "kwargs"], "sentSegmentedWithoutStops": ["inputs", "outputs", "updates=updates", "kwargs"], "sentSegmentedWithoutStopsStemmed": ["input", "output", "updates=upd", "kwarg"]}, {"number": 1175, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "  File \"/homes/sjebbara/git/keras-original/keras/backend/theano_backend.py\", line 773, in __init__", "sentSegmented": ["file", "/homes/sjebbara/git/keras-original/keras/backend/theano_backend.py", "line", "773", "in", "__init__"], "sentSegmentedWithoutStops": ["file", "/homes/sjebbara/git/keras-original/keras/backend/theano_backend.py", "line", "773", "__init__"], "sentSegmentedWithoutStopsStemmed": ["file", "/homes/sjebbara/git/keras-original/keras/backend/theano_backend.pi", "line", "773", "__init__"]}, {"number": 1176, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "    **kwargs)", "sentSegmented": ["kwargs"], "sentSegmentedWithoutStops": ["kwargs"], "sentSegmentedWithoutStopsStemmed": ["kwarg"]}, {"number": 1177, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "  File \"/homes/sjebbara/.local/lib/python2.7/site-packages/Theano-0.9.0.dev3-py2.7.egg/theano/compile/function.py\", line 326, in function", "sentSegmented": ["file", "/homes/sjebbara/.local/lib/python2.7/site-packages/theano-0.9.0.dev3-py2.7.egg/theano/compile/function.py", "line", "326", "in", "function"], "sentSegmentedWithoutStops": ["file", "/homes/sjebbara/.local/lib/python2.7/site-packages/theano-0.9.0.dev3-py2.7.egg/theano/compile/function.py", "line", "326"], "sentSegmentedWithoutStopsStemmed": ["file", "/homes/sjebbara/.local/lib/python2.7/site-packages/theano-0.9.0.dev3-py2.7.egg/theano/compile/function.pi", "line", "326"]}, {"number": 1178, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "    output_keys=output_keys)", "sentSegmented": ["output_keys=output_keys"], "sentSegmentedWithoutStops": ["output_keys=output_keys"], "sentSegmentedWithoutStopsStemmed": ["output_keys=output_key"]}, {"number": 1179, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "  File \"/homes/sjebbara/.local/lib/python2.7/site-packages/Theano-0.9.0.dev3-py2.7.egg/theano/compile/pfunc.py\", line 486, in pfunc", "sentSegmented": ["file", "/homes/sjebbara/.local/lib/python2.7/site-packages/theano-0.9.0.dev3-py2.7.egg/theano/compile/pfunc.py", "line", "486", "in", "pfunc"], "sentSegmentedWithoutStops": ["file", "/homes/sjebbara/.local/lib/python2.7/site-packages/theano-0.9.0.dev3-py2.7.egg/theano/compile/pfunc.py", "line", "486", "pfunc"], "sentSegmentedWithoutStopsStemmed": ["file", "/homes/sjebbara/.local/lib/python2.7/site-packages/theano-0.9.0.dev3-py2.7.egg/theano/compile/pfunc.pi", "line", "486", "pfunc"]}, {"number": 1180, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "    output_keys=output_keys)", "sentSegmented": ["output_keys=output_keys"], "sentSegmentedWithoutStops": ["output_keys=output_keys"], "sentSegmentedWithoutStopsStemmed": ["output_keys=output_key"]}, {"number": 1181, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "  File \"/homes/sjebbara/.local/lib/python2.7/site-packages/Theano-0.9.0.dev3-py2.7.egg/theano/compile/function_module.py\", line 1776, in orig_function", "sentSegmented": ["file", "/homes/sjebbara/.local/lib/python2.7/site-packages/theano-0.9.0.dev3-py2.7.egg/theano/compile/function_module.py", "line", "1776", "in", "orig_function"], "sentSegmentedWithoutStops": ["file", "/homes/sjebbara/.local/lib/python2.7/site-packages/theano-0.9.0.dev3-py2.7.egg/theano/compile/function_module.py", "line", "1776", "orig_function"], "sentSegmentedWithoutStopsStemmed": ["file", "/homes/sjebbara/.local/lib/python2.7/site-packages/theano-0.9.0.dev3-py2.7.egg/theano/compile/function_module.pi", "line", "1776", "orig_funct"]}, {"number": 1182, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "    output_keys=output_keys).create(", "sentSegmented": ["output_keys=output_keys", ".create"], "sentSegmentedWithoutStops": ["output_keys=output_keys", ".create"], "sentSegmentedWithoutStopsStemmed": ["output_keys=output_key", ".creat"]}, {"number": 1183, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "  File \"/homes/sjebbara/.local/lib/python2.7/site-packages/Theano-0.9.0.dev3-py2.7.egg/theano/compile/function_module.py\", line 1430, in __init__", "sentSegmented": ["file", "/homes/sjebbara/.local/lib/python2.7/site-packages/theano-0.9.0.dev3-py2.7.egg/theano/compile/function_module.py", "line", "1430", "in", "__init__"], "sentSegmentedWithoutStops": ["file", "/homes/sjebbara/.local/lib/python2.7/site-packages/theano-0.9.0.dev3-py2.7.egg/theano/compile/function_module.py", "line", "1430", "__init__"], "sentSegmentedWithoutStopsStemmed": ["file", "/homes/sjebbara/.local/lib/python2.7/site-packages/theano-0.9.0.dev3-py2.7.egg/theano/compile/function_module.pi", "line", "1430", "__init__"]}, {"number": 1184, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "    accept_inplace)", "sentSegmented": ["accept_inplace"], "sentSegmentedWithoutStops": ["accept_inplace"], "sentSegmentedWithoutStopsStemmed": ["accept_inplac"]}, {"number": 1185, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "  File \"/homes/sjebbara/.local/lib/python2.7/site-packages/Theano-0.9.0.dev3-py2.7.egg/theano/compile/function_module.py\", line 176, in std_fgraph", "sentSegmented": ["file", "/homes/sjebbara/.local/lib/python2.7/site-packages/theano-0.9.0.dev3-py2.7.egg/theano/compile/function_module.py", "line", "176", "in", "std_fgraph"], "sentSegmentedWithoutStops": ["file", "/homes/sjebbara/.local/lib/python2.7/site-packages/theano-0.9.0.dev3-py2.7.egg/theano/compile/function_module.py", "line", "176", "std_fgraph"], "sentSegmentedWithoutStopsStemmed": ["file", "/homes/sjebbara/.local/lib/python2.7/site-packages/theano-0.9.0.dev3-py2.7.egg/theano/compile/function_module.pi", "line", "176", "std_fgraph"]}, {"number": 1186, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "    update_mapping=update_mapping)", "sentSegmented": ["update_mapping=update_mapping"], "sentSegmentedWithoutStops": ["update_mapping=update_mapping"], "sentSegmentedWithoutStopsStemmed": ["update_mapping=update_map"]}, {"number": 1187, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "  File \"/homes/sjebbara/.local/lib/python2.7/site-packages/Theano-0.9.0.dev3-py2.7.egg/theano/gof/fg.py\", line 180, in __init__", "sentSegmented": ["file", "/homes/sjebbara/.local/lib/python2.7/site-packages/theano-0.9.0.dev3-py2.7.egg/theano/gof/fg.py", "line", "180", "in", "__init__"], "sentSegmentedWithoutStops": ["file", "/homes/sjebbara/.local/lib/python2.7/site-packages/theano-0.9.0.dev3-py2.7.egg/theano/gof/fg.py", "line", "180", "__init__"], "sentSegmentedWithoutStopsStemmed": ["file", "/homes/sjebbara/.local/lib/python2.7/site-packages/theano-0.9.0.dev3-py2.7.egg/theano/gof/fg.pi", "line", "180", "__init__"]}, {"number": 1188, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "    self.__import_r__(output, reason=\"init\")", "sentSegmented": ["self.__import_r__", "output", "reason=", "init"], "sentSegmentedWithoutStops": ["self.__import_r__", "output", "reason=", "init"], "sentSegmentedWithoutStopsStemmed": ["self.__import_r__", "output", "reason=", "init"]}, {"number": 1189, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "  File \"/homes/sjebbara/.local/lib/python2.7/site-packages/Theano-0.9.0.dev3-py2.7.egg/theano/gof/fg.py\", line 351, in __import_r__", "sentSegmented": ["file", "/homes/sjebbara/.local/lib/python2.7/site-packages/theano-0.9.0.dev3-py2.7.egg/theano/gof/fg.py", "line", "351", "in", "__import_r__"], "sentSegmentedWithoutStops": ["file", "/homes/sjebbara/.local/lib/python2.7/site-packages/theano-0.9.0.dev3-py2.7.egg/theano/gof/fg.py", "line", "351", "__import_r__"], "sentSegmentedWithoutStopsStemmed": ["file", "/homes/sjebbara/.local/lib/python2.7/site-packages/theano-0.9.0.dev3-py2.7.egg/theano/gof/fg.pi", "line", "351", "__import_r__"]}, {"number": 1190, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "    self.__import__(variable.owner, reason=reason)", "sentSegmented": ["self.__import__", "variable.owner", "reason=reason"], "sentSegmentedWithoutStops": ["self.__import__", "variable.owner", "reason=reason"], "sentSegmentedWithoutStopsStemmed": ["self.__import__", "variable.own", "reason=reason"]}, {"number": 1191, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "  File \"/homes/sjebbara/.local/lib/python2.7/site-packages/Theano-0.9.0.dev3-py2.7.egg/theano/gof/fg.py\", line 396, in __import__", "sentSegmented": ["file", "/homes/sjebbara/.local/lib/python2.7/site-packages/theano-0.9.0.dev3-py2.7.egg/theano/gof/fg.py", "line", "396", "in", "__import__"], "sentSegmentedWithoutStops": ["file", "/homes/sjebbara/.local/lib/python2.7/site-packages/theano-0.9.0.dev3-py2.7.egg/theano/gof/fg.py", "line", "396", "__import__"], "sentSegmentedWithoutStopsStemmed": ["file", "/homes/sjebbara/.local/lib/python2.7/site-packages/theano-0.9.0.dev3-py2.7.egg/theano/gof/fg.pi", "line", "396", "__import__"]}, {"number": 1192, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "    variable=r)", "sentSegmented": ["variable=r"], "sentSegmentedWithoutStops": ["variable=r"], "sentSegmentedWithoutStopsStemmed": ["variable=r"]}, {"number": 1193, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "theano.gof.fg.MissingInputError: An input of the graph, used to compute Shape(), was not provided and not given a value.Use the Theano flag exception_verbosity='high',for more information on this error.", "sentSegmented": ["theano.gof.fg.missinginputerror", "an", "input", "of", "the", "graph", "used", "to", "compute", "shape", "was", "not", "provided", "and", "not", "given", "a", "value.use", "the", "theano", "flag", "exception_verbosity='high", "for", "more", "information", "on", "this", "error"], "sentSegmentedWithoutStops": ["theano.gof.fg.missinginputerror", "input", "graph", "used", "compute", "shape", "provided", "given", "value.use", "theano", "flag", "exception_verbosity='high", "information", "error"], "sentSegmentedWithoutStopsStemmed": ["theano.gof.fg.missinginputerror", "input", "graph", "use", "comput", "shape", "provid", "given", "value.us", "theano", "flag", "exception_verbosity='high", "inform", "error"]}, {"number": 1194, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "Backtrace when the variable is created:", "sentSegmented": ["backtrace", "when", "the", "variable", "is", "created"], "sentSegmentedWithoutStops": ["backtrace", "variable", "created"], "sentSegmentedWithoutStopsStemmed": ["backtrac", "variabl", "creat"]}, {"number": 1195, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "  File \"/homes/sjebbara/PyCharmProjects/NeuralSentiment/src/Test2.py\", line 5, in ", "sentSegmented": ["file", "/homes/sjebbara/pycharmprojects/neuralsentiment/src/test2.py", "line", "5", "in"], "sentSegmentedWithoutStops": ["file", "/homes/sjebbara/pycharmprojects/neuralsentiment/src/test2.py", "line", "5"], "sentSegmentedWithoutStopsStemmed": ["file", "/homes/sjebbara/pycharmprojects/neuralsentiment/src/test2.pi", "line", "5"]}, {"number": 1196, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "    out1 = TimeDistributed(Dropout(0.5))(in1)", "sentSegmented": ["out1", "timedistributed", "dropout", "0.5", "in1"], "sentSegmentedWithoutStops": ["out1", "timedistributed", "dropout", "0.5", "in1"], "sentSegmentedWithoutStopsStemmed": ["out1", "timedistribut", "dropout", "0.5", "in1"]}, {"number": 1197, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "  File \"/homes/sjebbara/git/keras-original/keras/engine/topology.py\", line 514, in __call__", "sentSegmented": ["file", "/homes/sjebbara/git/keras-original/keras/engine/topology.py", "line", "514", "in", "__call__"], "sentSegmentedWithoutStops": ["file", "/homes/sjebbara/git/keras-original/keras/engine/topology.py", "line", "514", "__call__"], "sentSegmentedWithoutStopsStemmed": ["file", "/homes/sjebbara/git/keras-original/keras/engine/topology.pi", "line", "514", "__call__"]}, {"number": 1198, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "    self.add_inbound_node(inbound_layers, node_indices, tensor_indices)", "sentSegmented": ["self.add_inbound_node", "inbound_layers", "node_indices", "tensor_indices"], "sentSegmentedWithoutStops": ["self.add_inbound_node", "inbound_layers", "node_indices", "tensor_indices"], "sentSegmentedWithoutStopsStemmed": ["self.add_inbound_nod", "inbound_lay", "node_indic", "tensor_indic"]}, {"number": 1199, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "  File \"/homes/sjebbara/git/keras-original/keras/engine/topology.py\", line 572, in add_inbound_node", "sentSegmented": ["file", "/homes/sjebbara/git/keras-original/keras/engine/topology.py", "line", "572", "in", "add_inbound_node"], "sentSegmentedWithoutStops": ["file", "/homes/sjebbara/git/keras-original/keras/engine/topology.py", "line", "572", "add_inbound_node"], "sentSegmentedWithoutStopsStemmed": ["file", "/homes/sjebbara/git/keras-original/keras/engine/topology.pi", "line", "572", "add_inbound_nod"]}, {"number": 1200, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "    Node.create_node(self, inbound_layers, node_indices, tensor_indices)", "sentSegmented": ["node.create_node", "self", "inbound_layers", "node_indices", "tensor_indices"], "sentSegmentedWithoutStops": ["node.create_node", "self", "inbound_layers", "node_indices", "tensor_indices"], "sentSegmentedWithoutStopsStemmed": ["node.create_nod", "self", "inbound_lay", "node_indic", "tensor_indic"]}, {"number": 1201, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "  File \"/homes/sjebbara/git/keras-original/keras/engine/topology.py\", line 149, in create_node", "sentSegmented": ["file", "/homes/sjebbara/git/keras-original/keras/engine/topology.py", "line", "149", "in", "create_node"], "sentSegmentedWithoutStops": ["file", "/homes/sjebbara/git/keras-original/keras/engine/topology.py", "line", "149", "create_node"], "sentSegmentedWithoutStopsStemmed": ["file", "/homes/sjebbara/git/keras-original/keras/engine/topology.pi", "line", "149", "create_nod"]}, {"number": 1202, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "    output_tensors = to_list(outbound_layer.call(input_tensors[0], mask=input_masks[0]))", "sentSegmented": ["output_tensors", "to_list", "outbound_layer.call", "input_tensors", "0", "mask=input_masks", "0"], "sentSegmentedWithoutStops": ["output_tensors", "to_list", "outbound_layer.call", "input_tensors", "0", "mask=input_masks", "0"], "sentSegmentedWithoutStopsStemmed": ["output_tensor", "to_list", "outbound_layer.cal", "input_tensor", "0", "mask=input_mask", "0"]}, {"number": 1203, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "  File \"/homes/sjebbara/git/keras-original/keras/layers/wrappers.py\", line 131, in call", "sentSegmented": ["file", "/homes/sjebbara/git/keras-original/keras/layers/wrappers.py", "line", "131", "in", "call"], "sentSegmentedWithoutStops": ["file", "/homes/sjebbara/git/keras-original/keras/layers/wrappers.py", "line", "131", "call"], "sentSegmentedWithoutStopsStemmed": ["file", "/homes/sjebbara/git/keras-original/keras/layers/wrappers.pi", "line", "131", "call"]}, {"number": 1204, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "    initial_states=[], input_length=input_length, unroll=unroll)", "sentSegmented": ["initial_states=", "input_length=input_length", "unroll=unroll"], "sentSegmentedWithoutStops": ["initial_states=", "input_length=input_length", "unroll=unroll"], "sentSegmentedWithoutStopsStemmed": ["initial_states=", "input_length=input_length", "unroll=unrol"]}, {"number": 1205, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "  File \"/homes/sjebbara/git/keras-original/keras/backend/theano_backend.py\", line 947, in rnn", "sentSegmented": ["file", "/homes/sjebbara/git/keras-original/keras/backend/theano_backend.py", "line", "947", "in", "rnn"], "sentSegmentedWithoutStops": ["file", "/homes/sjebbara/git/keras-original/keras/backend/theano_backend.py", "line", "947", "rnn"], "sentSegmentedWithoutStopsStemmed": ["file", "/homes/sjebbara/git/keras-original/keras/backend/theano_backend.pi", "line", "947", "rnn"]}, {"number": 1206, "isCode": true, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "    go_backwards=go_backwards)", "sentSegmented": ["go_backwards=go_backwards"], "sentSegmentedWithoutStops": ["go_backwards=go_backwards"], "sentSegmentedWithoutStopsStemmed": ["go_backwards=go_backward"]}, {"number": 1207, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "Please make sure that the boxes below are checked before you submit your issue.", "sentSegmented": ["please", "make", "sure", "that", "the", "boxes", "below", "are", "checked", "before", "you", "submit", "your", "issue"], "sentSegmentedWithoutStops": ["please", "make", "sure", "boxes", "checked", "submit", "issue"], "sentSegmentedWithoutStopsStemmed": ["pleas", "make", "sure", "box", "check", "submit", "issu"]}, {"number": 1208, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "Thank you!", "sentSegmented": ["thank", "you"], "sentSegmentedWithoutStops": ["thank"], "sentSegmentedWithoutStopsStemmed": ["thank"]}, {"number": 1209, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "[ x] Check that you are up-to-date with the master branch of Keras.", "sentSegmented": ["x", "check", "that", "you", "are", "up-to-date", "with", "the", "master", "branch", "of", "keras"], "sentSegmentedWithoutStops": ["x", "check", "up-to-date", "master", "branch", "keras"], "sentSegmentedWithoutStopsStemmed": ["x", "check", "up-to-d", "master", "branch", "kera"]}, {"number": 1210, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "You can update with:", "sentSegmented": ["you", "can", "update", "with"], "sentSegmentedWithoutStops": ["update"], "sentSegmentedWithoutStopsStemmed": ["updat"]}, {"number": 1211, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps", "sentSegmented": ["pip", "install", "git+git", "//github.com/fchollet/keras.git", "upgrade", "no-deps"], "sentSegmentedWithoutStops": ["pip", "install", "git+git", "//github.com/fchollet/keras.git", "upgrade", "no-deps"], "sentSegmentedWithoutStopsStemmed": ["pip", "instal", "git+git", "//github.com/fchollet/keras.git", "upgrad", "no-dep"]}, {"number": 1212, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "[x ] If running on Theano, check that you are up-to-date with the master branch of Theano.", "sentSegmented": ["x", "if", "running", "on", "theano", "check", "that", "you", "are", "up-to-date", "with", "the", "master", "branch", "of", "theano"], "sentSegmentedWithoutStops": ["x", "running", "theano", "check", "up-to-date", "master", "branch", "theano"], "sentSegmentedWithoutStopsStemmed": ["x", "run", "theano", "check", "up-to-d", "master", "branch", "theano"]}, {"number": 1213, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "You can update with:", "sentSegmented": ["you", "can", "update", "with"], "sentSegmentedWithoutStops": ["update"], "sentSegmentedWithoutStopsStemmed": ["updat"]}, {"number": 1214, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps", "sentSegmented": ["pip", "install", "git+git", "//github.com/theano/theano.git", "upgrade", "no-deps"], "sentSegmentedWithoutStops": ["pip", "install", "git+git", "//github.com/theano/theano.git", "upgrade", "no-deps"], "sentSegmentedWithoutStopsStemmed": ["pip", "instal", "git+git", "//github.com/theano/theano.git", "upgrad", "no-dep"]}, {"number": 1215, "isCode": false, "isBlockQuote": false, "blockQuoteDepth": 0, "sent": "[x ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).", "sentSegmented": ["x", "provide", "a", "link", "to", "a", "github", "gist", "of", "a", "python", "script", "that", "can", "reproduce", "your", "issue", "or", "just", "copy", "the", "script", "here", "if", "it", "is", "short"], "sentSegmentedWithoutStops": ["x", "provide", "link", "github", "gist", "python", "script", "reproduce", "issue", "copy", "script", "short"], "sentSegmentedWithoutStopsStemmed": ["x", "provid", "link", "github", "gist", "python", "script", "reproduc", "issu", "copi", "script", "short"]}]}